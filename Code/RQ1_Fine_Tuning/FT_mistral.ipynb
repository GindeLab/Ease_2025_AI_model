{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Availability: True\n",
      "CUDA is available. Device: NVIDIA GeForce RTX 4090\n",
      "\n",
      "CUDA Version:\n",
      "nvcc not found. Make sure NVIDIA CUDA Toolkit is installed.\n",
      "\n",
      "NVIDIA-SMI Output:\n",
      "Sun Mar  2 03:03:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 33%   59C    P2            168W /  450W |   17099MiB /  24564MiB |     21%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       155      C   /ollama                                     N/A      |\n",
      "|    0   N/A  N/A      1435      C   /python3.11                                 N/A      |\n",
      "|    0   N/A  N/A      6615      C   /python3.11                                 N/A      |\n",
      "|    0   N/A  N/A     11244      C   /python3.11                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_cuda_status():\n",
    "    \"\"\"\n",
    "    Retrieves and displays CUDA status information using `nvidia-smi`.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the output of `nvidia-smi`, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error executing nvidia-smi: {e.stderr}\"\n",
    "    except FileNotFoundError:\n",
    "        return \"nvidia-smi not found. Make sure NVIDIA drivers and CUDA are installed.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "\n",
    "def get_cuda_version():\n",
    "    \"\"\"\n",
    "    Retrieves the CUDA version using `nvcc --version`.\n",
    "\n",
    "    Returns:\n",
    "        str: The CUDA version string, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, check=True)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error executing nvcc --version: {e.stderr}\"\n",
    "    except FileNotFoundError:\n",
    "        return \"nvcc not found. Make sure NVIDIA CUDA Toolkit is installed.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "def check_cuda_availability():\n",
    "    \"\"\"\n",
    "    Checks if CUDA is available by attempting to import torch.cuda (if torch is installed)\n",
    "    or by checking for nvidia-smi.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if CUDA is likely available, False otherwise.\n",
    "        str: A string with more details.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return True, f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\"\n",
    "        else:\n",
    "            return False, \"CUDA is not available according to torch.\"\n",
    "    except ImportError:\n",
    "        # torch is not installed, fall back to nvidia-smi\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return True, \"CUDA is likely available (nvidia-smi found), but torch is not installed to confirm.\"\n",
    "        else:\n",
    "            return False, \"CUDA is not available (nvidia-smi not found).\"\n",
    "    except Exception as e:\n",
    "        return False, f\"An unexpected error occurred while checking CUDA availability: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    available, availability_message = check_cuda_availability()\n",
    "    print(f\"CUDA Availability: {available}\")\n",
    "    print(availability_message)\n",
    "\n",
    "    print(\"\\nCUDA Version:\")\n",
    "    print(get_cuda_version())\n",
    "\n",
    "    print(\"\\nNVIDIA-SMI Output:\")\n",
    "    print(get_cuda_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache cleaned (Linux/macOS): /home/jags/.nv/ComputeCache\n",
      "CUDA cache directory not found (Linux/macOS): /home/jags/.cache/nvidia/GLCache\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def clean_cuda_cache():\n",
    "    \"\"\"\n",
    "    Attempts to clean the CUDA cache by deleting the shader cache directories.\n",
    "    This is a platform-specific operation and might require administrator privileges.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':  # Windows\n",
    "            cache_dir = os.path.join(os.environ.get('LOCALAPPDATA'), 'NVIDIA', 'DXCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['powershell', '-Command', f'Remove-Item -Recurse -Force \"{cache_dir}\"'], check=True)\n",
    "                print(f\"CUDA cache cleaned (Windows): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Windows): {cache_dir}\")\n",
    "\n",
    "            cache_dir = os.path.join(os.environ.get('LOCALAPPDATA'), 'NVIDIA', 'GLCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['powershell', '-Command', f'Remove-Item -Recurse -Force \"{cache_dir}\"'], check=True)\n",
    "                print(f\"CUDA cache cleaned (Windows): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Windows): {cache_dir}\")\n",
    "\n",
    "        elif os.name == 'posix':  # Linux/macOS\n",
    "            cache_dir = os.path.expanduser('~/.nv/ComputeCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['rm', '-rf', cache_dir], check=True)\n",
    "                print(f\"CUDA cache cleaned (Linux/macOS): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Linux/macOS): {cache_dir}\")\n",
    "\n",
    "            cache_dir = os.path.expanduser('~/.cache/nvidia/GLCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['rm', '-rf', cache_dir], check=True)\n",
    "                print(f\"CUDA cache cleaned (Linux/macOS): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Linux/macOS): {cache_dir}\")\n",
    "\n",
    "        else:\n",
    "            print(\"Unsupported operating system.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cleaning CUDA cache: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"rm or powershell not found\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied when trying to delete the cache. You may need administrator privileges.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_cuda_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prerequisite. Run on Terminal. Install miniconda. Navigate to the folder cd /home/asus/\n",
    "\n",
    "# mkdir -p ~/miniconda3\n",
    "# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "# bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "# rm -rf ~/miniconda3/miniconda.sh\n",
    "# ~/miniconda3/bin/conda init bash\n",
    "# ~/miniconda3/bin/conda init zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUn on terminal: Create a virtual environment, Install all the requirements like pytorch, xformers, cuda etc.\n",
    "#### install the unsloth library and other libraries viz. peft\n",
    "\n",
    "# conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "# conda activate unsloth_env\n",
    "# pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: OpenAI failed to import - ignoring for now.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.8: Fast Mistral patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that\n",
    "includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    " Highlight the missing information to the reporter: Explicitly notify the user which information is missing.\n",
    "\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for summary, report in zip(examples[\"LLAMA Output\"], examples[\"Input\"]):\n",
    "        formatted_text = alpaca_prompt.format(Summary=summary,Response=report) + EOS_TOKEN\n",
    "        texts.append(formatted_text)\n",
    "    return {\"text\": texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    \n",
    "    for summary, report in zip(examples[\"NEW_llama_output\"], examples[\"text\"]):\n",
    "        # Create the conversation in the required JSON format\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in generating detailed bug reports.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"### Instruction:\n",
    "Please create a bug report that\n",
    "includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter: Explicitly notify the user which information is missing.\n",
    "\n",
    "### Context:\n",
    "{summary}\"\"\"},\n",
    "            {\"role\": \"assistant\", \"content\": report}\n",
    "        ]\n",
    "        \n",
    "        # Convert each message to a JSON string and join with newlines\n",
    "        formatted_text = \"\\n\".join(json.dumps(message) for message in conversation)\n",
    "        texts.append(formatted_text)\n",
    "    \n",
    "    return {\"train_text\": texts}\n",
    "\n",
    "# Define the EOS token to be used with the tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt= \"\"\"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\n",
    "\n",
    "# ### sql_prompt:\n",
    "# {}\n",
    "\n",
    "# ### sql:\n",
    "# {}\n",
    "\n",
    "# ### Explanation:\n",
    "# {}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an assistant specialized in generating detailed bug reports.\\n\\n### Instruction:\\nPlease create a bug report that\\nincludes the following sections:\\n1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\\n2. Expected Result (ER): What you expected to happen.\\n3. Actual Result (AR): What actually happened.\\n4. Additional Information: Include relevant details such as software version, build number, environment, etc.\\n Highlight the missing information to the reporter: Explicitly notify the user which information is missing.\\n\\n### Context:\\n{Summary}\\n\\n### Response:\\n{Response}\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_prompts_func(examples):\n",
    "#     sql_prompt = examples[\"sql_prompt\"]\n",
    "#     sql = examples[\"sql\"]\n",
    "#     sql_explanation = examples['sql_explanation']\n",
    "\n",
    "#     texts = []\n",
    "    \n",
    "#     EOS_TOKEN = tokenizer.eos_token\n",
    "#     for sql_prompt, sql, sql_explanation in zip(sql_prompt, sql, sql_explanation):\n",
    "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#         text = alpaca_prompt.format(sql_prompt, sql, sql_explanation) + EOS_TOKEN\n",
    "#         texts.append(text)\n",
    "#     return { \"text\" : texts, }\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'user_behviou', 'total_score', 'max_possible', 'score_percentage', 'RM1_size_passed', 'RM1_size_score', 'RM2_readability_passed', 'RM2_readability_score', 'RM3_punctuation_passed', 'RM3_punctuation_score', 'RM4_sentence_length_passed', 'RM4_sentence_length_score', 'RR1_itemization_passed', 'RR1_itemization_score', 'RR2_itemization_symbol_passed', 'RR2_itemization_symbol_score', 'RR3_environment_passed', 'RR3_environment_score', 'RR4_screenshot_passed', 'RR4_screenshot_score', 'RR5_screenshot_guideline_passed', 'RR5_screenshot_guideline_score', 'RA1_interface_element_passed', 'RA1_interface_element_score', 'RA2_user_behavior_passed', 'RA2_user_behavior_score', 'RA3_system_defect_passed', 'RA3_system_defect_score', 'RA4_defect_description_passed', 'RA4_defect_description_score', 'NEW_llama_output']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"jag2023/Ultimate_CTQRS_good_BG\", split = \"train\")\n",
    "test_dataset = load_dataset(\"jag2023/Ultimate_CTQRS_good_BG\", split = \"test\")\n",
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2781\n",
      "Test dataset size: 147\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"jag2023/CTQRS_dataset_final\", split=\"train\")\n",
    "\n",
    "# Split the dataset into 90% train and 10% test\n",
    "split_dataset = dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "# Extract the train and test datasets\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "# Display the sizes of train and test datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'user_behviou', 'total_score', 'max_possible', 'score_percentage', 'RM1_size_passed', 'RM1_size_score', 'RM2_readability_passed', 'RM2_readability_score', 'RM3_punctuation_passed', 'RM3_punctuation_score', 'RM4_sentence_length_passed', 'RM4_sentence_length_score', 'RR1_itemization_passed', 'RR1_itemization_score', 'RR2_itemization_symbol_passed', 'RR2_itemization_symbol_score', 'RR3_environment_passed', 'RR3_environment_score', 'RR4_screenshot_passed', 'RR4_screenshot_score', 'RR5_screenshot_guideline_passed', 'RR5_screenshot_guideline_score', 'RA1_interface_element_passed', 'RA1_interface_element_score', 'RA2_user_behavior_passed', 'RA2_user_behavior_score', 'RA3_system_defect_passed', 'RA3_system_defect_score', 'RA4_defect_description_passed', 'RA4_defect_description_score', 'NEW_llama_output'],\n",
       "    num_rows: 3172\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86da6eb2333d40b6a7424c31e197b881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = train_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['NEW_llama_output', 'raw_text', 'total_score', 'text'],\n",
       "    num_rows: 2781\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"role\": \"system\", \"content\": \"You are an assistant specialized in generating detailed bug reports.\"}\\n{\"role\": \"user\", \"content\": \"### Instruction:\\\\nPlease create a bug report that\\\\nincludes the following sections:\\\\n1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\\\\n2. Expected Result (ER): What you expected to happen.\\\\n3. Actual Result (AR): What actually happened.\\\\n4. Additional Information: Include relevant details such as software version, build number, environment, etc.\\\\nHighlight the missing information to the reporter: Explicitly notify the user which information is missing.\\\\n\\\\n### Context:\\\\nHere\\'s the rewritten bug report:\\\\n\\\\n\\\\\"So, I\\'m using Firefox in French and trying to right-click on an image on a webpage - nothing out of the ordinary. But when I press \\'s\\', which is supposed to open up that context menu with all sorts of options, it just doesn\\'t do anything. And if you look closely, there are two items in that menu that both have the letter \\'s\\' as their shortcut key: \\'Enregistrer l\\'image sous\\' (Save image as) and \\'Choisir l\\'image comme fond de l\\'\\\\u00e9cran\\' (Choose image as background). The thing is, when I try to use the first one - Save image as - it just doesn\\'t work. It\\'s like it gets stuck or something. I was expecting that option to pop up when I pressed \\'s\\', but instead nothing happens.\\\\\"\"}\\n{\"role\": \"assistant\", \"content\": \"User Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0\\\\n\\\\nSteps to reproduce:\\\\n\\\\nUsing Firefox in French, right-click an image on a web page, press \\\\\"s\\\\\". \\\\n\\\\n\\\\nActual results:\\\\n\\\\nNothing happens. There seem to be 2 items in the context menu that have the same shortcut letter \\\\\"s\\\\\". \\\\n- Enregistrer l\\'image sous (Save image as) \\\\n- Choisir l\\'image comme fond de l\\'\\\\u00e9cran (Choose image as background)\\\\n\\\\nThe item Enregistrer l\\'image sous (Save image as) is not executed.\\\\n\\\\n\\\\nExpected results:\\\\n\\\\nThe item Enregistrer l\\'image sous (Save image as) should be executed.\"}']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89bd82c904640ec8c64ffd1bece0253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/3172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"train_text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 3,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps =  120,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.988 GB.\n",
      "4.031 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 3,172 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 2 | Total steps = 1,586\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:382\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/_utils.py:1069\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1067\u001b[0m     )\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1130\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1129\u001b[0m ):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/mistral.py:221\u001b[0m, in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m LlamaModel_fast_forward_inference(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    215\u001b[0m         input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    235\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:821\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offloaded_gradient_checkpointing:\n\u001b[0;32m--> 821\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mUnsloth_Offloaded_Gradient_Checkpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m gradient_checkpointing:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_custom_forward\u001b[39m(module):\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:465\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/gradient_checkpointing.py:145\u001b[0m, in \u001b[0;36mUnsloth_Offloaded_Gradient_Checkpointer.forward\u001b[0;34m(ctx, forward_function, hidden_states, *args)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;129m@torch_amp_custom_fwd\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(ctx, forward_function, hidden_states, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 145\u001b[0m     saved_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    147\u001b[0m         output \u001b[38;5;241m=\u001b[39m forward_function(hidden_states, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087.205 seconds used for training.\n",
      "18.12 minutes used for training.\n",
      "Peak reserved memory = 8.195 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 34.163 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "test_dataset = load_dataset(\"jag2023/CTQRS_test\", split=\"test\")\n",
    "\n",
    "# # Split the dataset into 90% train and 10% test\n",
    "# split_dataset = dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "# # Extract the train and test datasets\n",
    "\n",
    "# train_dataset = split_dataset['train']\n",
    "# test_dataset = split_dataset['test']\n",
    "\n",
    "# # Display the sizes of train and test datasets\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id_x', 'count', 'id', 'tags', 'reactions', 'is_private', 'raw_text', 'creation_time_x', 'text', 'bug_id', 'attachment_id', 'creator_x', 'time', 'author', 'Bug report', 'author_id', '_id_y', 'severity', 'summary', 'votes', 'cf_tracking_firefox_esr128', 'qa_contact', 'cf_fx_points', 'cf_last_resolved', 'duplicates', 'see_also', 'cf_tracking_thunderbird_esr128', 'cf_tracking_thunderbird_esr115', 'cc_detail', 'cf_qa_whiteboard', 'blocks', 'cf_webcompat_score', 'product', 'flags', 'alias', 'creation_time_y', 'cf_has_str', 'mentors', 'creator_detail', 'cf_status_thunderbird_esr128', 'cf_crash_signature', 'op_sys', 'cf_accessibility_severity', 'target_milestone', 'url', 'cf_user_story', 'cf_tracking_firefox_relnote', 'cf_webcompat_priority', 'priority', 'cf_fx_iteration', 'cf_a11y_review_project_flag', 'is_open', 'cf_tracking_firefox132', 'status', 'cf_cab_review', 'cf_status_firefox134', 'assigned_to_detail', 'cf_tracking_firefox134', 'version', 'keywords', 'cf_status_firefox_esr115', 'is_confirmed', 'mentors_detail', 'component', 'cc', 'comment_count', 'cf_performance_impact', 'cf_status_firefox132', 'type', 'is_cc_accessible', 'dupe_of', 'regressed_by', 'whiteboard', 'platform', 'cf_status_firefox133', 'cf_tracking_firefox133', 'groups', 'cf_tracking_firefox_esr115', 'creator_y', 'cf_status_firefox_esr128', 'resolution', 'last_change_time', 'assigned_to', 'cf_status_thunderbird_esr115', 'classification', 'cf_rank', 'depends_on', 'regressions', 'is_creator_accessible', 'Contributor_email', 'Contributor_Id', 'cf_status_firefox_esr102', 'cf_status_firefox109', 'cf_status_firefox108', 'cf_status_firefox107', 'cf_status_firefox118', 'cf_status_firefox119', 'cf_tracking_firefox119', 'cf_tracking_firefox120', 'cf_status_firefox120', 'cf_blocking_webextensions', 'cf_tracking_thunderbird_134', 'cf_tracking_thunderbird_relnote', 'cf_tracking_thunderbird_132', 'cf_status_thunderbird_132', 'cf_status_thunderbird_133', 'cf_tracking_thunderbird_133', 'cf_status_thunderbird_134', 'cf_tracking_firefox109', 'cf_status_firefox_esr78', 'cf_status_firefox88', 'cf_status_thunderbird_esr78', 'cf_status_firefox89', 'cf_status_firefox87', 'cf_install_update_workflow', 'cf_tracking_firefox89', 'cf_status_firefox86', 'cf_due_date', 'cf_tracking_bmo_push', 'cf_status_firefox110', 'cf_fission_milestone', 'cf_locale', 'cf_status_firefox111', 'cf_status_firefox90', 'cf_status_firefox91', 'cf_status_firefox112', 'cf_status_seamonkey257esr', 'cf_tracking_seamonkey257esr', 'cf_status_seamonkey253', 'cf_status_thunderbird_88', 'cf_tracking_seamonkey253', 'cf_status_firefox114', 'cf_status_firefox115', 'cf_status_firefox113', 'cf_status_firefox117', 'cf_status_firefox97', 'qa_contact_detail', 'cf_status_thunderbird_89', 'cf_tracking_thunderbird_esr78', 'cf_tracking_firefox107', 'cf_tracking_conduit_push', 'cf_status_conduit_push', 'cf_status_firefox129', 'cf_status_firefox131', 'cf_status_firefox130', 'cf_status_firefox94', 'cf_status_firefox96', 'cf_status_firefox_esr91', 'cf_status_firefox95', 'cf_status_thunderbird_esr102', 'cf_status_thunderbird_109', 'cf_status_firefox116', 'cf_tracking_firefox88', 'cf_tracking_firefox108', 'cf_tracking_firefox_esr102', 'cf_status_thunderbird_108', 'cf_status_bmo_push', 'cf_tracking_thunderbird_108', 'cf_tracking_thunderbird_esr102', 'cf_status_firefox123', 'cf_status_thunderbird_115', 'cf_tracking_firefox110', 'cf_status_thunderbird_esr91', 'cf_status_thunderbird_99', 'cf_status_thunderbird_111', 'cf_status_firefox93', 'cf_status_firefox92', 'cf_status_thunderbird_87', 'cf_status_firefox100', 'cf_tracking_firefox87', 'cf_tracking_firefox_esr78', 'cf_status_firefox121', 'cf_status_firefox122', 'cf_status_firefox103', 'cf_status_firefox104', 'cf_status_firefox126', 'cf_tracking_firefox90', 'cf_tracking_firefox91', 'cf_status_thunderbird_123', 'cf_status_firefox128', 'cf_tracking_fxios', 'cf_tracking_firefox121', 'cf_status_firefox125', 'cf_status_firefox106', 'cf_status_firefox105', 'cf_status_firefox124', 'cf_status_thunderbird_90', 'cf_status_thunderbird_91', 'cf_status_thunderbird_121', 'cf_status_thunderbird_122', 'cf_tracking_firefox111', 'cf_status_firefox99', 'cf_status_firefox101', 'cf_status_firefox102', 'cf_status_firefox98', 'cf_tracking_thunderbird_88', 'cf_status_thunderbird_107', 'cf_tracking_firefox86', 'cf_status_thunderbird_93', 'cf_status_thunderbird_94', 'cf_tracking_thunderbird_esr91', 'cf_tracking_firefox_esr91', 'cf_status_thunderbird_96', 'cf_status_thunderbird_97', 'cf_tracking_firefox112', 'cf_status_thunderbird_110', 'cf_status_thunderbird_86', 'cf_tracking_firefox_sumo', 'cf_status_thunderbird_105', 'cf_status_firefox127', 'cf_status_thunderbird_101', 'cf_tracking_firefox106', 'cf_tracking_thunderbird_115', 'cf_tracking_firefox100', 'cf_tracking_firefox101', 'cf_tracking_firefox95', 'cf_tracking_thunderbird_87', 'cf_status_thunderbird_100', 'cf_tracking_thunderbird_102', 'cf_status_thunderbird_102', 'cf_tracking_firefox97', 'cf_tracking_firefox98', 'cf_tracking_firefox96', 'cf_status_thunderbird_103', 'cf_status_thunderbird_104', 'cf_tracking_thunderbird_91', 'cf_status_thunderbird_98', 'cf_tracking_firefox99', 'cf_tracking_thunderbird_97', 'cf_status_firefox85', 'cf_colo_site', 'cf_tracking_thunderbird_90', 'cf_tracking_firefox102', 'cf_tracking_thunderbird_93', 'cf_tracking_thunderbird_96', 'cf_status_thunderbird_113', 'cf_tracking_thunderbird_86', 'cf_status_thunderbird_85', 'cf_tracking_thunderbird_101', 'cf_status_thunderbird_120', 'cf_status_thunderbird_119', 'cf_status_thunderbird_106', 'cf_tracking_firefox85', 'cf_tracking_firefox113', 'cf_mozilla_project', 'cf_status_thunderbird_92', 'cf_tracking_firefox105', 'cf_tracking_thunderbird_131', 'cf_status_thunderbird_114', 'cf_tracking_thunderbird_130', 'cf_status_firefox84', 'cf_tracking_firefox103', 'cf_status_thunderbird_117', 'cf_status_thunderbird_116', 'cf_status_thunderbird_112', 'cf_tracking_thunderbird_100', 'cf_tracking_thunderbird_99', 'cf_tracking_firefox94', 'cf_tracking_firefox93', 'cf_status_thunderbird_95', 'cf_tracking_thunderbird_94', 'cf_status_thunderbird_118', 'cf_tracking_firefox92', 'cf_tracking_firefox127', 'cf_tracking_thunderbird_106', 'cf_tracking_thunderbird_95', 'cf_status_thunderbird_124', 'cf_tracking_thunderbird_105', 'cf_tracking_firefox104', 'cf_tracking_thunderbird_116', 'cf_tracking_thunderbird_92', 'cf_tracking_thunderbird_103', 'cf_tracking_thunderbird_128', 'cf_status_thunderbird_128', 'cf_tracking_thunderbird_122', 'cf_tracking_thunderbird_109', 'cf_tracking_firefox128', 'cf_tracking_firefox126', 'cf_tracking_firefox122', 'cf_tracking_thunderbird_104', 'cf_tracking_firefox115', 'cf_tracking_firefox116', 'cf_tracking_firefox117', 'cf_status_thunderbird_129', 'cf_tracking_firefox124', 'cf_tracking_thunderbird_111', 'cf_tracking_thunderbird_89', 'cf_root_cause', 'cf_tracking_firefox114', 'cf_tracking_firefox118', 'cf_tracking_thunderbird_119', 'cf_status_thunderbird_131', 'cf_tracking_firefox129', 'cf_tracking_thunderbird_127', 'cf_status_thunderbird_127', 'cf_status_thunderbird_126', 'cf_status_thunderbird_130', 'cf_status_thunderbird_125', 'cf_tracking_thunderbird_126', 'cf_tracking_firefox125', 'cf_tracking_thunderbird_125', 'cf_tracking_thunderbird_124', 'cf_tracking_firefox123', 'cf_tracking_thunderbird_129', 'cf_tracking_thunderbird_123', 'cf_tracking_thunderbird_121', 'cf_tracking_thunderbird_120', 'cf_tracking_thunderbird_118', 'cf_tracking_thunderbird_117', 'cf_tracking_thunderbird_114', 'cf_tracking_thunderbird_113', 'cf_tracking_thunderbird_112', 'cf_tracking_thunderbird_110', 'cf_tracking_firefox130', 'contains_steps_to_reproduce', 'total_score', 'max_possible', 'score_percentage', 'RM1_size_passed', 'RM1_size_score', 'RM2_readability_passed', 'RM2_readability_score', 'RM3_punctuation_passed', 'RM3_punctuation_score', 'RM4_sentence_length_passed', 'RM4_sentence_length_score', 'RR1_itemization_passed', 'RR1_itemization_score', 'RR2_itemization_symbol_passed', 'RR2_itemization_symbol_score', 'RR3_environment_passed', 'RR3_environment_score', 'RR4_screenshot_passed', 'RR4_screenshot_score', 'RR5_screenshot_guideline_passed', 'RR5_screenshot_guideline_score', 'RA1_interface_element_passed', 'RA1_interface_element_score', 'RA2_user_behavior_passed', 'RA2_user_behavior_score', 'RA3_system_defect_passed', 'RA3_system_defect_score', 'RA4_defect_description_passed', 'RA4_defect_description_score', 'Bug ID', 'Comment ID', 'Author', 'Comment Text', 'Contains Steps to Reproduce', 'Contains Actual Result', 'Contains Expected Result or Expect', 'Contains Expected', 'Contains Actual', 'Steps to Reproduce', 'Expected_Res', 'Actual_Res', 'Summary', 'Product', '__index_level_0__', 'NEW_llama_output'],\n",
       "    num_rows: 1027\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "index 7\n",
      "index 8\n",
      "index 9\n",
      "index 10\n",
      "index 11\n",
      "index 12\n",
      "index 13\n",
      "index 14\n",
      "index 15\n",
      "index 16\n",
      "index 17\n",
      "index 18\n",
      "index 19\n",
      "index 20\n",
      "index 21\n",
      "index 22\n",
      "index 23\n",
      "index 24\n",
      "index 25\n",
      "index 26\n",
      "index 27\n",
      "index 28\n",
      "index 29\n",
      "index 30\n",
      "index 31\n",
      "index 32\n",
      "index 33\n",
      "index 34\n",
      "index 35\n",
      "index 36\n",
      "index 37\n",
      "index 38\n",
      "index 39\n",
      "index 40\n",
      "index 41\n",
      "index 42\n",
      "index 43\n",
      "index 44\n",
      "index 45\n",
      "index 46\n",
      "index 47\n",
      "index 48\n",
      "index 49\n",
      "index 50\n",
      "index 51\n",
      "index 52\n",
      "index 53\n",
      "index 54\n",
      "index 55\n",
      "index 56\n",
      "index 57\n",
      "index 58\n",
      "index 59\n",
      "index 60\n",
      "index 61\n",
      "index 62\n",
      "index 63\n",
      "index 64\n",
      "index 65\n",
      "index 66\n",
      "index 67\n",
      "index 68\n",
      "index 69\n",
      "index 70\n",
      "index 71\n",
      "index 72\n",
      "index 73\n",
      "index 74\n",
      "index 75\n",
      "index 76\n",
      "index 77\n",
      "index 78\n",
      "index 79\n",
      "index 80\n",
      "index 81\n",
      "index 82\n",
      "index 83\n",
      "index 84\n",
      "index 85\n",
      "index 86\n",
      "index 87\n",
      "index 88\n",
      "index 89\n",
      "index 90\n",
      "index 91\n",
      "index 92\n",
      "index 93\n",
      "index 94\n",
      "index 95\n",
      "index 96\n",
      "index 97\n",
      "index 98\n",
      "index 99\n",
      "index 100\n",
      "index 101\n",
      "index 102\n",
      "index 103\n",
      "index 104\n",
      "index 105\n",
      "index 106\n",
      "index 107\n",
      "index 108\n",
      "index 109\n",
      "index 110\n",
      "index 111\n",
      "index 112\n",
      "index 113\n",
      "index 114\n",
      "index 115\n",
      "index 116\n",
      "index 117\n",
      "index 118\n",
      "index 119\n",
      "index 120\n",
      "index 121\n",
      "index 122\n",
      "index 123\n",
      "index 124\n",
      "index 125\n",
      "index 126\n",
      "index 127\n",
      "index 128\n",
      "index 129\n",
      "index 130\n",
      "index 131\n",
      "index 132\n",
      "index 133\n",
      "index 134\n",
      "index 135\n",
      "index 136\n",
      "index 137\n",
      "index 138\n",
      "index 139\n",
      "index 140\n",
      "index 141\n",
      "index 142\n",
      "index 143\n",
      "index 144\n",
      "index 145\n",
      "index 146\n",
      "index 147\n",
      "index 148\n",
      "index 149\n",
      "index 150\n",
      "index 151\n",
      "index 152\n",
      "index 153\n",
      "index 154\n",
      "index 155\n",
      "index 156\n",
      "index 157\n",
      "index 158\n",
      "index 159\n",
      "index 160\n",
      "index 161\n",
      "index 162\n",
      "index 163\n",
      "index 164\n",
      "index 165\n",
      "index 166\n",
      "index 167\n",
      "index 168\n",
      "index 169\n",
      "index 170\n",
      "index 171\n",
      "index 172\n",
      "index 173\n",
      "index 174\n",
      "index 175\n",
      "index 176\n",
      "index 177\n",
      "index 178\n",
      "index 179\n",
      "index 180\n",
      "index 181\n",
      "index 182\n",
      "index 183\n",
      "index 184\n",
      "index 185\n",
      "index 186\n",
      "index 187\n",
      "index 188\n",
      "index 189\n",
      "index 190\n",
      "index 191\n",
      "index 192\n",
      "index 193\n",
      "index 194\n",
      "index 195\n",
      "index 196\n",
      "index 197\n",
      "index 198\n",
      "index 199\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an senior engineer specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report with proper itemization and it should includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = test_dataset.to_pandas()\n",
    "dataset= dataset[:200]\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['NEW_llama_output']\n",
    "    actual_report = row['raw_text']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune mistral Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"CTQRS_200_Score_test_mistral.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4891\n",
      "Validation dataset size: 611\n",
      "Test dataset size: 612\n",
      "\n",
      "Column names of the train dataset:\n",
      "['Input', 'LLAMA Output', 'SBERT', 'BLEU', 'ROUGE', 'METEOR', 'Jaccard', 'Cosine']\n"
     ]
    }
   ],
   "source": [
    "# # !pip install markdownfrom datasets import load_dataset, DatasetDict\n",
    "\n",
    "# # Load the initial dataset\n",
    "# dataset = load_dataset(\"jag2023/llama_bug_rep_Summary-dataset\", split=\"train\")\n",
    "\n",
    "# # Perform the first split to get 80% train and 20% (test + validation)\n",
    "# train_test_dataset = dataset.train_test_split(test_size=0.2, seed=42) # Setting a seed for reproducibility\n",
    "\n",
    "# # Split the 20% (test + validation) into 10% test and 10% validation\n",
    "# test_valid_dataset = train_test_dataset['test'].train_test_split(test_size=0.5, seed=42) # Using the same seed for consistency\n",
    "\n",
    "# # Create the final train, validation, and test datasets\n",
    "# train_dataset = train_test_dataset['train']\n",
    "# validation_dataset = test_valid_dataset['train']\n",
    "# test_dataset = test_valid_dataset['test']\n",
    "\n",
    "# # Print dataset information to verify the splits (optional but good practice)\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "# print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# # Print column names for the train dataset (as in the original code)\n",
    "# print(\"\\nColumn names of the train dataset:\")\n",
    "# print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"27_feb_8pm_ollama_summary_RQ2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df2\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Summary']\n",
    "    actual_report = row['Actual Report']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune llama Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"top_good50_mapping_study.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>NEW_llama_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>created attachment 9303641\\nscreenshot\\n\\n[tra...</td>\n",
       "      <td>I was trying to use the Japanese UI build of F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>I was trying to download Ubuntu for my ARM-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I was trying to use a new feature on my app, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>created attachment 9303207\\nfirefox-typed-slug...</td>\n",
       "      <td>Here's the bug report transformed into a conci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steps to reproduce:\\n\\n1. export .vcf file fro...</td>\n",
       "      <td>I'm still fuming about this frustrating experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>created attachment 9302717\\nscrfra1.pdf\\n\\nuse...</td>\n",
       "      <td>I'm still frustrated about this issue I had wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>created attachment 9210007\\nclose tabs.gif\\n\\n...</td>\n",
       "      <td>So, I was using Firefox on my Mac, Windows, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>Here's the rewritten paragraph:\\n\\nI'm still s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>I was working in a Windows HCM theme and I wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>**affected versions**\\n* latest nightly 88.0a1...</td>\n",
       "      <td>Here's the rewritten paragraph:\\n\\nI'm still t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>created attachment 9207492\\nscreenshot of bug ...</td>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>I'm still fuming about this frustrating issue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>created attachment 9206584\\nexample.pdf\\n\\nuse...</td>\n",
       "      <td>Here's a rewritten version of your bug report ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 6.1; win64...</td>\n",
       "      <td>Here's the rewritten paragraph:\\n\\nI was tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>created attachment 9206144\\nthe pdf file that ...</td>\n",
       "      <td>I was trying to open a PDF file from the FAA w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>So, I was trying to troubleshoot this weird is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>created attachment 9205668\\nwikipedia ticks in...</td>\n",
       "      <td>I was trying to use the \"Find on this page...\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>created attachment 9260072\\ntroubleshooting-in...</td>\n",
       "      <td>I've been dealing with this frustrating issue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>created attachment 9205464\\nscreenshot 2021-02...</td>\n",
       "      <td>I was trying to create a large background with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>created attachment 9259844\\nblack screen betwe...</td>\n",
       "      <td>I'm still fuming about this annoying issue I'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>created attachment 9259764\\nfirefox bug report...</td>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>So I was messing around with my Firefox settin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; ubuntu; linux x8...</td>\n",
       "      <td>I was trying to add a CalDAV network calendar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>created attachment 9204875\\nscreenshot_19.jpg\\...</td>\n",
       "      <td>Here's a rewritten version of your bug report ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>Here's what happened: I went to this website, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>I was excited to try out the new features in T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>created attachment 9203980\\nstrictvsstandard.p...</td>\n",
       "      <td>Here's my attempt to transform your bug report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>steps to reproduce:\\n\\nuser agent: mozilla/5.0...</td>\n",
       "      <td>So I was messing around with this online code ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>Here's my take on the bug report:\\n\\nI was try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>Here's the transformed paragraph:\\n\\nSo, I was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>created attachment 9203398\\nthunderbird_qeu8sv...</td>\n",
       "      <td>Here's my attempt to transform the bug report ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>created attachment 9299706\\nright-click menu i...</td>\n",
       "      <td>I'm still scratching my head about this bizarr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>created attachment 9203346\\nthe picture of the...</td>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>created attachment 9299610\\n2022-10-21 024326....</td>\n",
       "      <td>Here's a paragraph rephrased from your bug rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>I'm still frustrated about this annoying issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>created attachment 9257783\\nimage (2).png\\n\\nu...</td>\n",
       "      <td>I was trying to play a browser game I love, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>Here's the bug report rewritten in a conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>created attachment 9202889\\nscreenshot 2021-02...</td>\n",
       "      <td>I was trying to view a webpage with some Unico...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>created attachment 9299257\\nthe left pane does...</td>\n",
       "      <td>I was trying to work with some data in the dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>I was trying to open Firefox from my Gnome She...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>created attachment 9257274\\nprivate window\\n\\n...</td>\n",
       "      <td>I'll never forget the frustration I experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; ubuntu; linux x8...</td>\n",
       "      <td>Here's a rewritten bug report in a concise, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>created attachment 9298817\\nscreenshot_2022101...</td>\n",
       "      <td>I'm still fuming about this frustrating experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>Here's the rewritten paragraph:\\n\\nI was tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>I'm still fuming about this weird issue I ran ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>Here's the bug report transformed into a conve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>I was trying to manage my bookmarks, but I hit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>created attachment 9298559\\nsearchbar minimum ...</td>\n",
       "      <td>I was trying to customize my Firefox browser, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>created attachment 9201816\\nbubble_pop_00000.a...</td>\n",
       "      <td>I was trying to open a file called \"bubble_pop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   created attachment 9303641\\nscreenshot\\n\\n[tra...   \n",
       "1   user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "2                                                 NaN   \n",
       "3   created attachment 9303207\\nfirefox-typed-slug...   \n",
       "4   steps to reproduce:\\n\\n1. export .vcf file fro...   \n",
       "5   created attachment 9302717\\nscrfra1.pdf\\n\\nuse...   \n",
       "6   created attachment 9210007\\nclose tabs.gif\\n\\n...   \n",
       "7   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "8   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "9   **affected versions**\\n* latest nightly 88.0a1...   \n",
       "10  created attachment 9207492\\nscreenshot of bug ...   \n",
       "11  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "12  created attachment 9206584\\nexample.pdf\\n\\nuse...   \n",
       "13  user agent: mozilla/5.0 (windows nt 6.1; win64...   \n",
       "14  created attachment 9206144\\nthe pdf file that ...   \n",
       "15  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "16  created attachment 9205668\\nwikipedia ticks in...   \n",
       "17  created attachment 9260072\\ntroubleshooting-in...   \n",
       "18  created attachment 9205464\\nscreenshot 2021-02...   \n",
       "19  created attachment 9259844\\nblack screen betwe...   \n",
       "20  created attachment 9259764\\nfirefox bug report...   \n",
       "21  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "22  user agent: mozilla/5.0 (x11; ubuntu; linux x8...   \n",
       "23  created attachment 9204875\\nscreenshot_19.jpg\\...   \n",
       "24  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "25  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "26  created attachment 9203980\\nstrictvsstandard.p...   \n",
       "27  steps to reproduce:\\n\\nuser agent: mozilla/5.0...   \n",
       "28  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "29  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "30  created attachment 9203398\\nthunderbird_qeu8sv...   \n",
       "31  created attachment 9299706\\nright-click menu i...   \n",
       "32  created attachment 9203346\\nthe picture of the...   \n",
       "33  created attachment 9299610\\n2022-10-21 024326....   \n",
       "34  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "35  created attachment 9257783\\nimage (2).png\\n\\nu...   \n",
       "36  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "37  created attachment 9202889\\nscreenshot 2021-02...   \n",
       "38  created attachment 9299257\\nthe left pane does...   \n",
       "39  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "40  created attachment 9257274\\nprivate window\\n\\n...   \n",
       "41  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "42  user agent: mozilla/5.0 (x11; ubuntu; linux x8...   \n",
       "43  created attachment 9298817\\nscreenshot_2022101...   \n",
       "44  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "45  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "46  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "47  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "48  created attachment 9298559\\nsearchbar minimum ...   \n",
       "49  created attachment 9201816\\nbubble_pop_00000.a...   \n",
       "\n",
       "                                     NEW_llama_output  \n",
       "0   I was trying to use the Japanese UI build of F...  \n",
       "1   I was trying to download Ubuntu for my ARM-bas...  \n",
       "2   I was trying to use a new feature on my app, s...  \n",
       "3   Here's the bug report transformed into a conci...  \n",
       "4   I'm still fuming about this frustrating experi...  \n",
       "5   I'm still frustrated about this issue I had wi...  \n",
       "6   So, I was using Firefox on my Mac, Windows, an...  \n",
       "7   Here's the rewritten paragraph:\\n\\nI'm still s...  \n",
       "8   I was working in a Windows HCM theme and I wan...  \n",
       "9   Here's the rewritten paragraph:\\n\\nI'm still t...  \n",
       "10  Here's my attempt at transforming the bug repo...  \n",
       "11  I'm still fuming about this frustrating issue ...  \n",
       "12  Here's a rewritten version of your bug report ...  \n",
       "13  Here's the rewritten paragraph:\\n\\nI was tryin...  \n",
       "14  I was trying to open a PDF file from the FAA w...  \n",
       "15  So, I was trying to troubleshoot this weird is...  \n",
       "16  I was trying to use the \"Find on this page...\"...  \n",
       "17  I've been dealing with this frustrating issue ...  \n",
       "18  I was trying to create a large background with...  \n",
       "19  I'm still fuming about this annoying issue I'v...  \n",
       "20  Here's my attempt at transforming the bug repo...  \n",
       "21  So I was messing around with my Firefox settin...  \n",
       "22  I was trying to add a CalDAV network calendar ...  \n",
       "23  Here's a rewritten version of your bug report ...  \n",
       "24  Here's what happened: I went to this website, ...  \n",
       "25  I was excited to try out the new features in T...  \n",
       "26  Here's my attempt to transform your bug report...  \n",
       "27  So I was messing around with this online code ...  \n",
       "28  Here's my take on the bug report:\\n\\nI was try...  \n",
       "29  Here's the transformed paragraph:\\n\\nSo, I was...  \n",
       "30  Here's my attempt to transform the bug report ...  \n",
       "31  I'm still scratching my head about this bizarr...  \n",
       "32  Here's my attempt at transforming the bug repo...  \n",
       "33  Here's a paragraph rephrased from your bug rep...  \n",
       "34  I'm still frustrated about this annoying issue...  \n",
       "35  I was trying to play a browser game I love, bu...  \n",
       "36  Here's the bug report rewritten in a conversat...  \n",
       "37  I was trying to view a webpage with some Unico...  \n",
       "38  I was trying to work with some data in the dev...  \n",
       "39  I was trying to open Firefox from my Gnome She...  \n",
       "40  I'll never forget the frustration I experience...  \n",
       "41  Here's my attempt at transforming the bug repo...  \n",
       "42  Here's a rewritten bug report in a concise, co...  \n",
       "43  I'm still fuming about this frustrating experi...  \n",
       "44  Here's the rewritten paragraph:\\n\\nI was tryin...  \n",
       "45  I'm still fuming about this weird issue I ran ...  \n",
       "46  Here's the bug report transformed into a conve...  \n",
       "47  I was trying to manage my bookmarks, but I hit...  \n",
       "48  I was trying to customize my Firefox browser, ...  \n",
       "49  I was trying to open a file called \"bubble_pop...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "index 7\n",
      "index 8\n",
      "index 9\n",
      "index 10\n",
      "index 11\n",
      "index 12\n",
      "index 13\n",
      "index 14\n",
      "index 15\n",
      "index 16\n",
      "index 17\n",
      "index 18\n",
      "index 19\n",
      "index 20\n",
      "index 21\n",
      "index 22\n",
      "index 23\n",
      "index 24\n",
      "index 25\n",
      "index 26\n",
      "index 27\n",
      "index 28\n",
      "index 29\n",
      "index 30\n",
      "index 31\n",
      "index 32\n",
      "index 33\n",
      "index 34\n",
      "index 35\n",
      "index 36\n",
      "index 37\n",
      "index 38\n",
      "index 39\n",
      "index 40\n",
      "index 41\n",
      "index 42\n",
      "index 43\n",
      "index 44\n",
      "index 45\n",
      "index 46\n",
      "index 47\n",
      "index 48\n",
      "index 49\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df2\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Summary']\n",
    "    actual_report = row['Actual Report']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune llama Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"top_good50_mapping_study.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"top_good50_mapping_study.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Actual Report</th>\n",
       "      <th>Fine_tune llama Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was trying to use the Japanese UI build of F...</td>\n",
       "      <td>created attachment 9303641\\nscreenshot\\n\\n[tra...</td>\n",
       "      <td>**Bug 1779695:** Introduce modern flex on in-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was trying to download Ubuntu for my ARM-bas...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>**Bug Report**\\n\\n**1. Steps to Reproduce (S2R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was trying to use a new feature on my app, s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>created attachment 9303207\\nfirefox-typed-slug...</td>\n",
       "      <td>**Bug ID:** [To be assigned]\\n\\n**Affected Ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm still fuming about this frustrating experi...</td>\n",
       "      <td>steps to reproduce:\\n\\n1. export .vcf file fro...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nuser agent: mozilla/5.0 (windows nt 10.0; wi...</td>\n",
       "      <td>created attachment 9302717\\nscrfra1.pdf\\n\\nuse...</td>\n",
       "      <td>**Bug ID:** [To be assigned]\\n\\n**Software Ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>So, I was using Firefox on my Mac, Windows, an...</td>\n",
       "      <td>created attachment 9210007\\nclose tabs.gif\\n\\n...</td>\n",
       "      <td>**Bug Report**\\n\\n**Platform:** macOS 10.15.7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>**Bug Report**\\n\\n**Title:** Firefox browser d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>**Bug Report**\\n\\n**Title:** Date and Time con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Here's the rewritten paragraph:\\n\\nI'm still t...</td>\n",
       "      <td>**affected versions**\\n* latest nightly 88.0a1...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>created attachment 9207492\\nscreenshot of bug ...</td>\n",
       "      <td>**Steps to Reproduce (S2R):**\\n\\n1. Open a Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Here's a rewritten version of your bug report ...</td>\n",
       "      <td>created attachment 9206584\\nexample.pdf\\n\\nuse...</td>\n",
       "      <td>To address this issue, I've tried opening the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 6.1; win64...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 6.1; win64...</td>\n",
       "      <td>**Bug Report**\\n\\n**Software:** OpenPGP Key Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I was trying to open a PDF file from the FAA w...</td>\n",
       "      <td>created attachment 9206144\\nthe pdf file that ...</td>\n",
       "      <td>I can confirm that the issue you're experienci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\nuser agent: mozilla/5.0 (x11; ubuntu; linux ...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I was trying to use the \"Find on this page...\"...</td>\n",
       "      <td>created attachment 9205668\\nwikipedia ticks in...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>created attachment 9260072\\ntroubleshooting-in...</td>\n",
       "      <td>**Bug Report**\\n\\n**User Agent:** Mozilla/5.0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I was trying to create a large background with...</td>\n",
       "      <td>created attachment 9205464\\nscreenshot 2021-02...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I'm still fuming about this annoying issue I'v...</td>\n",
       "      <td>created attachment 9259844\\nblack screen betwe...</td>\n",
       "      <td>**Bug Report**\\n\\n\\n**Steps to Reproduce (S2R)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "      <td>created attachment 9259764\\nfirefox bug report...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I was trying to add a CalDAV network calendar ...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; ubuntu; linux x8...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>**environment:**\\noperating system: ubuntu 20....</td>\n",
       "      <td>created attachment 9204875\\nscreenshot_19.jpg\\...</td>\n",
       "      <td>**Steps to Reproduce (S2R):**\\n\\n1. Open Firef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Here's what happened: I went to this website, ...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>**Bug Report**\\n\\n**Bug ID:** [Insert ID here]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Here's my attempt to transform your bug report...</td>\n",
       "      <td>created attachment 9203980\\nstrictvsstandard.p...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>steps to reproduce:\\n\\nuser agent: mozilla/5.0...</td>\n",
       "      <td>**Bug Report**\\n\\n**Bug ID:** [To be assigned]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Here's my take on the bug report:\\n\\nI was try...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>#### Steps to Reproduce (S2R):\\n\\n1. Open Fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Here's the transformed paragraph:\\n\\nSo, I was...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>To reproduce the issue, follow these steps:\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\\nuser agent: mozilla/5.0 (windows nt 10.0; wi...</td>\n",
       "      <td>created attachment 9203398\\nthunderbird_qeu8sv...</td>\n",
       "      <td>**Bug Report**\\n\\n\\n**Summary:** Messages in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; ubuntu; linux x8...</td>\n",
       "      <td>created attachment 9299706\\nright-click menu i...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "      <td>created attachment 9203346\\nthe picture of the...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>created attachment 9299610\\n2022-10-21 024326....</td>\n",
       "      <td>#### Steps to Reproduce (S2R):\\n1. Go to Windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>**Bug ID:** 1714574\\n\\n**Affected Versions:** ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>created attachment 9257783\\nimage (2).png\\n\\nu...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>Here's the bug report rewritten in a conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>created attachment 9202889\\nscreenshot 2021-02...</td>\n",
       "      <td>**Bug Report**\\n\\n**1. Steps to Reproduce (S2R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>created attachment 9299257\\nthe left pane does...</td>\n",
       "      <td>I apologize for the inconvenience you are expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I was trying to open Firefox from my Gnome She...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>To report the issue with the Turkish translati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I'll never forget the frustration I experience...</td>\n",
       "      <td>created attachment 9257274\\nprivate window\\n\\n...</td>\n",
       "      <td>**Bug ID:** [Insert ID here]\\n\\n**Component:**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Here's my attempt at transforming the bug repo...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>user agent: mozilla/5.0 (x11; ubuntu; linux x8...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; ubuntu; linux x8...</td>\n",
       "      <td>**Bug Report**\\n\\n**1. Steps to Reproduce (S2R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I'm still fuming about this frustrating experi...</td>\n",
       "      <td>created attachment 9298817\\nscreenshot_2022101...</td>\n",
       "      <td>**Bug Report**\\n\\n**Bug ID:** 1798570\\n\\n**Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Here's the rewritten paragraph:\\n\\nI was tryin...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>#### Steps to Reproduce (S2R):\\n\\n1. Open a fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>user agent: mozilla/5.0 (macintosh; intel mac ...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>I was trying to manage my bookmarks, but I hit...</td>\n",
       "      <td>user agent: mozilla/5.0 (x11; linux x86_64; rv...</td>\n",
       "      <td>**Bug Report**\\n\\n\\n**Steps to Reproduce (S2R)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>user agent: mozilla/5.0 (windows nt 10.0; win6...</td>\n",
       "      <td>created attachment 9298559\\nsearchbar minimum ...</td>\n",
       "      <td>**Bug Report**\\n\\n**Bug ID:** [Insert ID here]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>\\nuser agent: mozilla/5.0 (windows nt 10.0; wi...</td>\n",
       "      <td>created attachment 9201816\\nbubble_pop_00000.a...</td>\n",
       "      <td>**Bug Report**\\n\\n**Steps to Reproduce (S2R):*...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Summary  \\\n",
       "0   I was trying to use the Japanese UI build of F...   \n",
       "1   I was trying to download Ubuntu for my ARM-bas...   \n",
       "2   I was trying to use a new feature on my app, s...   \n",
       "3   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "4   I'm still fuming about this frustrating experi...   \n",
       "5   \\nuser agent: mozilla/5.0 (windows nt 10.0; wi...   \n",
       "6   So, I was using Firefox on my Mac, Windows, an...   \n",
       "7   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "8   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "9   Here's the rewritten paragraph:\\n\\nI'm still t...   \n",
       "10  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "11  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "12  Here's a rewritten version of your bug report ...   \n",
       "13  user agent: mozilla/5.0 (windows nt 6.1; win64...   \n",
       "14  I was trying to open a PDF file from the FAA w...   \n",
       "15  \\nuser agent: mozilla/5.0 (x11; ubuntu; linux ...   \n",
       "16  I was trying to use the \"Find on this page...\"...   \n",
       "17  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "18  I was trying to create a large background with...   \n",
       "19  I'm still fuming about this annoying issue I'v...   \n",
       "20  Here's my attempt at transforming the bug repo...   \n",
       "21  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "22  I was trying to add a CalDAV network calendar ...   \n",
       "23  **environment:**\\noperating system: ubuntu 20....   \n",
       "24  Here's what happened: I went to this website, ...   \n",
       "25  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "26  Here's my attempt to transform your bug report...   \n",
       "27  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "28  Here's my take on the bug report:\\n\\nI was try...   \n",
       "29  Here's the transformed paragraph:\\n\\nSo, I was...   \n",
       "30  \\nuser agent: mozilla/5.0 (windows nt 10.0; wi...   \n",
       "31  user agent: mozilla/5.0 (x11; ubuntu; linux x8...   \n",
       "32  Here's my attempt at transforming the bug repo...   \n",
       "33  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "34  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "35  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "36  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "37  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "38  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "39  I was trying to open Firefox from my Gnome She...   \n",
       "40  I'll never forget the frustration I experience...   \n",
       "41  Here's my attempt at transforming the bug repo...   \n",
       "42  user agent: mozilla/5.0 (x11; ubuntu; linux x8...   \n",
       "43  I'm still fuming about this frustrating experi...   \n",
       "44  Here's the rewritten paragraph:\\n\\nI was tryin...   \n",
       "45  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "46  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "47  I was trying to manage my bookmarks, but I hit...   \n",
       "48  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "49  \\nuser agent: mozilla/5.0 (windows nt 10.0; wi...   \n",
       "\n",
       "                                        Actual Report  \\\n",
       "0   created attachment 9303641\\nscreenshot\\n\\n[tra...   \n",
       "1   user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "2                                                 NaN   \n",
       "3   created attachment 9303207\\nfirefox-typed-slug...   \n",
       "4   steps to reproduce:\\n\\n1. export .vcf file fro...   \n",
       "5   created attachment 9302717\\nscrfra1.pdf\\n\\nuse...   \n",
       "6   created attachment 9210007\\nclose tabs.gif\\n\\n...   \n",
       "7   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "8   user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "9   **affected versions**\\n* latest nightly 88.0a1...   \n",
       "10  created attachment 9207492\\nscreenshot of bug ...   \n",
       "11  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "12  created attachment 9206584\\nexample.pdf\\n\\nuse...   \n",
       "13  user agent: mozilla/5.0 (windows nt 6.1; win64...   \n",
       "14  created attachment 9206144\\nthe pdf file that ...   \n",
       "15  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "16  created attachment 9205668\\nwikipedia ticks in...   \n",
       "17  created attachment 9260072\\ntroubleshooting-in...   \n",
       "18  created attachment 9205464\\nscreenshot 2021-02...   \n",
       "19  created attachment 9259844\\nblack screen betwe...   \n",
       "20  created attachment 9259764\\nfirefox bug report...   \n",
       "21  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "22  user agent: mozilla/5.0 (x11; ubuntu; linux x8...   \n",
       "23  created attachment 9204875\\nscreenshot_19.jpg\\...   \n",
       "24  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "25  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "26  created attachment 9203980\\nstrictvsstandard.p...   \n",
       "27  steps to reproduce:\\n\\nuser agent: mozilla/5.0...   \n",
       "28  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "29  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "30  created attachment 9203398\\nthunderbird_qeu8sv...   \n",
       "31  created attachment 9299706\\nright-click menu i...   \n",
       "32  created attachment 9203346\\nthe picture of the...   \n",
       "33  created attachment 9299610\\n2022-10-21 024326....   \n",
       "34  user agent: mozilla/5.0 (windows nt 10.0; win6...   \n",
       "35  created attachment 9257783\\nimage (2).png\\n\\nu...   \n",
       "36  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "37  created attachment 9202889\\nscreenshot 2021-02...   \n",
       "38  created attachment 9299257\\nthe left pane does...   \n",
       "39  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "40  created attachment 9257274\\nprivate window\\n\\n...   \n",
       "41  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "42  user agent: mozilla/5.0 (x11; ubuntu; linux x8...   \n",
       "43  created attachment 9298817\\nscreenshot_2022101...   \n",
       "44  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "45  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "46  user agent: mozilla/5.0 (macintosh; intel mac ...   \n",
       "47  user agent: mozilla/5.0 (x11; linux x86_64; rv...   \n",
       "48  created attachment 9298559\\nsearchbar minimum ...   \n",
       "49  created attachment 9201816\\nbubble_pop_00000.a...   \n",
       "\n",
       "                               Fine_tune llama Output  \n",
       "0   **Bug 1779695:** Introduce modern flex on in-c...  \n",
       "1   **Bug Report**\\n\\n**1. Steps to Reproduce (S2R...  \n",
       "2   **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "3   **Bug ID:** [To be assigned]\\n\\n**Affected Ver...  \n",
       "4   **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "5   **Bug ID:** [To be assigned]\\n\\n**Software Ver...  \n",
       "6   **Bug Report**\\n\\n**Platform:** macOS 10.15.7,...  \n",
       "7   **Bug Report**\\n\\n**Title:** Firefox browser d...  \n",
       "8   **Bug Report**\\n\\n**Title:** Date and Time con...  \n",
       "9   **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "10  **Steps to Reproduce (S2R):**\\n\\n1. Open a Cod...  \n",
       "11  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "12  To address this issue, I've tried opening the ...  \n",
       "13  **Bug Report**\\n\\n**Software:** OpenPGP Key Ma...  \n",
       "14  I can confirm that the issue you're experienci...  \n",
       "15  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "16  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "17  **Bug Report**\\n\\n**User Agent:** Mozilla/5.0 ...  \n",
       "18  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "19  **Bug Report**\\n\\n\\n**Steps to Reproduce (S2R)...  \n",
       "20  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "21  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "22  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "23  **Steps to Reproduce (S2R):**\\n\\n1. Open Firef...  \n",
       "24  **Bug Report**\\n\\n**Bug ID:** [Insert ID here]...  \n",
       "25  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "26  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "27  **Bug Report**\\n\\n**Bug ID:** [To be assigned]...  \n",
       "28  #### Steps to Reproduce (S2R):\\n\\n1. Open Fire...  \n",
       "29  To reproduce the issue, follow these steps:\\n\\...  \n",
       "30  **Bug Report**\\n\\n\\n**Summary:** Messages in a...  \n",
       "31  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "32  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "33  #### Steps to Reproduce (S2R):\\n1. Go to Windo...  \n",
       "34  **Bug ID:** 1714574\\n\\n**Affected Versions:** ...  \n",
       "35  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "36  Here's the bug report rewritten in a conversat...  \n",
       "37  **Bug Report**\\n\\n**1. Steps to Reproduce (S2R...  \n",
       "38  I apologize for the inconvenience you are expe...  \n",
       "39  To report the issue with the Turkish translati...  \n",
       "40  **Bug ID:** [Insert ID here]\\n\\n**Component:**...  \n",
       "41  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "42  **Bug Report**\\n\\n**1. Steps to Reproduce (S2R...  \n",
       "43  **Bug Report**\\n\\n**Bug ID:** 1798570\\n\\n**Com...  \n",
       "44  #### Steps to Reproduce (S2R):\\n\\n1. Open a fr...  \n",
       "45  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "46  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  \n",
       "47  **Bug Report**\\n\\n\\n**Steps to Reproduce (S2R)...  \n",
       "48  **Bug Report**\\n\\n**Bug ID:** [Insert ID here]...  \n",
       "49  **Bug Report**\\n\\n**Steps to Reproduce (S2R):*...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd # Make sure you have pandas imported\n",
    "\n",
    "# def structure_llm_output(llm_output_text):\n",
    "#     \"\"\"\n",
    "#     Structures the LLM output text to extract sections for bug report.\n",
    "#     ... (rest of your structure_llm_output function code is the same) ...\n",
    "#     \"\"\"\n",
    "#     sections = {\n",
    "#         \"Steps to Reproduce\": None,\n",
    "#         \"Expected Result\": None,\n",
    "#         \"Actual Result\": None,\n",
    "#         \"Additional Information\": None,\n",
    "#     }\n",
    "#     section_headings = list(sections.keys())\n",
    "#     for i, section_name in enumerate(section_headings):\n",
    "#         next_section_heading = section_headings[i+1] if i+1 < len(section_headings) else None\n",
    "#         # start_pattern = re.compile(rf\"{re.escape(section_name)}:\\s*(.*?)(?=(?:\\n{re.escape(section_headings[i+1])}:)|\\Z)\", re.DOTALL)\n",
    "#         start_pattern = re.compile(rf\"{re.escape(section_name)}:\\s*(.*?)(?=(?:\\n{re.escape(section_headings[i+1])}:)|\\Z)\", re.DOTALL)\n",
    "#         match = start_pattern.search(llm_output_text)\n",
    "#         if match:\n",
    "#             sections[section_name] = match.group(1).strip()\n",
    "#     return sections\n",
    "\n",
    "# # Assuming df2 is your DataFrame and 'Fine_tune llama Output' is the column\n",
    "# # --- Process DataFrame ---\n",
    "\n",
    "# # 1. Create a new column to store the structured output (dictionaries)\n",
    "# df2['Structured_Output'] = None # Initialize a new column\n",
    "\n",
    "# # 2. Iterate through each row of df2 and apply structure_llm_output to the 'Fine_tune llama Output'\n",
    "# for index, row in df2.iterrows():\n",
    "#     llm_output_text = row['Fine_tune llama Output'] # Get the string from the current row\n",
    "#     if isinstance(llm_output_text, str): # Important: Check if it's actually a string\n",
    "#         structured_output_dict = structure_llm_output(llm_output_text)\n",
    "#         df2.at[index, 'Structured_Output'] = structured_output_dict # Store the dictionary back in the DataFrame\n",
    "#     else:\n",
    "#         df2.at[index, 'Structured_Output'] = None # Or handle non-string cases as needed, perhaps log a warning\n",
    "\n",
    "# # --- Example of accessing structured data ---\n",
    "# # After processing, each row in df2 will have a dictionary in the 'Structured_Output' column\n",
    "\n",
    "# # Example: Print structured output for the first few rows\n",
    "# print(\"Structured Output for DataFrame:\")\n",
    "# for index, row in df2.head().iterrows(): # Show for the first few rows\n",
    "#     structured_data = row['Structured_Output']\n",
    "#     if structured_data: # Check if structured data was successfully extracted\n",
    "#         print(f\"\\nRow Index: {index}\")\n",
    "#         print(\"Steps:\", structured_data.get(\"Steps to Reproduce\"))\n",
    "#         print(\"Expected:\", structured_data.get(\"Expected Result\"))\n",
    "#         print(\"Actual:\", structured_data.get(\"Actual Result\"))\n",
    "#         print(\"Additional Info:\", structured_data.get(\"Additional Information\"))\n",
    "#     else:\n",
    "#         print(f\"\\nRow Index: {index} - No structured output (perhaps input was not a string or parsing failed)\")\n",
    "#     print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Summary'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 34\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m     actual_report \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Report\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m     mistral_repo \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMistral Report\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Summary'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Summary']\n",
    "    actual_report = row['Actual Report']\n",
    "    mistral_repo = row['Mistral Report']\n",
    "    pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        'Mistral Report': mistral_repo,\n",
    "        'Fine_tune Output': generated_response,\n",
    "        'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"Full_Final_epoc_600_newrefine_llama_Fine_tine_output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "make: Leaving directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "-- The C compiler identification is GNU 13.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The CXX compiler identification is GNU 13.3.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.43.0\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring incomplete, errors occurred!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mCMake Error at /usr/share/cmake-3.28/Modules/FindPackageHandleStandardArgs.cmake:230 (message):\n",
      "  Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR)\n",
      "Call Stack (most recent call first):\n",
      "  /usr/share/cmake-3.28/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE)\n",
      "  /usr/share/cmake-3.28/Modules/FindCURL.cmake:194 (find_package_handle_standard_args)\n",
      "  common/CMakeLists.txt:88 (find_package)\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "*** Unsloth: Failed compiling llama.cpp using os.system(...) with error 256. Please report this ASAP!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/save.py:1684\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1682\u001b[0m python_install \u001b[38;5;241m=\u001b[39m install_python_non_blocking([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1683\u001b[0m git_clone\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m-> 1684\u001b[0m makefile \u001b[38;5;241m=\u001b[39m \u001b[43minstall_llama_cpp_make_non_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m new_save_directory, old_username \u001b[38;5;241m=\u001b[39m unsloth_save_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   1686\u001b[0m python_install\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/save.py:779\u001b[0m, in \u001b[0;36minstall_llama_cpp_make_non_blocking\u001b[0;34m()\u001b[0m\n\u001b[1;32m    777\u001b[0m check \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Unsloth: Failed compiling llama.cpp using os.system(...) with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please report this ASAP!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# f\"cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}\",\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: *** Unsloth: Failed compiling llama.cpp using os.system(...) with error 256. Please report this ASAP!"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this Steps for Local Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.06 out of 15.47 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:01<00:00, 28.82it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "make: Leaving directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n"
     ]
    }
   ],
   "source": [
    "!make clean -C llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "make: Leaving directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n"
     ]
    }
   ],
   "source": [
    "!make all -j -C llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gguf in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: protobuf in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (2.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (6.0.2)\n",
      "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gguf protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> Q8_0, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> Q8_0, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128255\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:jagunsloth_model_8.gguf: n_tensors = 291, total_size = 8.5G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.53G/8.53G [00:37<00:00, 229Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to jagunsloth_model_8.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py merged_model --outfile jagunsloth_model_8.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALL OLLAMA using Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      "[sudo] password for asus: \n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:285d51ea4f63810cddbc950e114a3f19c58990862cad7135443f0da6c8b9d17f 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 0% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:285d51ea4f63810cddbc950e114a3f19c58990862cad7135443f0da6c8b9d17f 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 100% \n",
      "copying file sha256:0a518f0766fc3484bbc16b6aa4eeec9fc94dad4eb8d30f4ac7759134bcb91a2c 100% \n",
      "copying file sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b 100% \n",
      "copying file sha256:6e1e79b7c44f96217478536d89869fc23c75f2e1f140e3149a9765c8ed1a6b57 100% \n",
      "copying file sha256:e7e4039387ae974f2130df2909b21335624a1bf421b256191ccd6a280a9db0fa 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:6e1e79b7c44f96217478536d89869fc23c75f2e1f140e3149a9765c8ed1a6b57 100% \n",
      "copying file sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:0a518f0766fc3484bbc16b6aa4eeec9fc94dad4eb8d30f4ac7759134bcb91a2c 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:22b3df42c4e872761824da0313e770fdd3641baf4a7808071145362cf8beeaae 100% \n",
      "copying file sha256:303a35b38c7e6a72988c09cc68f4e739b40ded80b0635191e16588b4c801e15e 100% \n",
      "copying file sha256:1b02cb2b24127ff1e955fe55447dabba3f683488e4edb4c2ff005f25333ac197 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 100% \n",
      "copying file sha256:0a518f0766fc3484bbc16b6aa4eeec9fc94dad4eb8d30f4ac7759134bcb91a2c 100% \n",
      "copying file sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:22b3df42c4e872761824da0313e770fdd3641baf4a7808071145362cf8beeaae 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:6e1e79b7c44f96217478536d89869fc23c75f2e1f140e3149a9765c8ed1a6b57 100% \n",
      "parsing GGUF \u001b[?25h\n",
      "Error: supplied file was not in GGUF format\n"
     ]
    }
   ],
   "source": [
    "!ollama create jags -f Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt1= \"\"\"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\n",
    "\n",
    "### sql_prompt:\n",
    "{}\n",
    "\n",
    "### sql:\n",
    "{}\n",
    "\n",
    "### Explanation:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = alpaca_prompt1.format(\n",
    "    \"What is the total volume of timber sold by each salesperson, sorted by salesperson?\",\n",
    "    \"\",\n",
    "    \"\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the total volume of timber sold by each salesperson, sorted by salesperson?\\n\\n### sql:\\n\\n\\n### Explanation:\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the average property size in inclusive housing areas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = alpaca_prompt1.format(\n",
    "    query,\n",
    "    \"\",\n",
    "    \"\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the average property size in inclusive housing areas?\\n\\n### sql:\\n\\n\\n### Explanation:\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
