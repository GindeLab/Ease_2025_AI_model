{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Availability: True\n",
      "CUDA is available. Device: NVIDIA GeForce RTX 4090\n",
      "\n",
      "CUDA Version:\n",
      "nvcc not found. Make sure NVIDIA CUDA Toolkit is installed.\n",
      "\n",
      "NVIDIA-SMI Output:\n",
      "Wed Mar  5 10:10:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 61%   66C    P2            225W /  450W |    8376MiB /  24564MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       155      C   /ollama                                     N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_cuda_status():\n",
    "    \"\"\"\n",
    "    Retrieves and displays CUDA status information using `nvidia-smi`.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the output of `nvidia-smi`, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error executing nvidia-smi: {e.stderr}\"\n",
    "    except FileNotFoundError:\n",
    "        return \"nvidia-smi not found. Make sure NVIDIA drivers and CUDA are installed.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "\n",
    "def get_cuda_version():\n",
    "    \"\"\"\n",
    "    Retrieves the CUDA version using `nvcc --version`.\n",
    "\n",
    "    Returns:\n",
    "        str: The CUDA version string, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, check=True)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error executing nvcc --version: {e.stderr}\"\n",
    "    except FileNotFoundError:\n",
    "        return \"nvcc not found. Make sure NVIDIA CUDA Toolkit is installed.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "def check_cuda_availability():\n",
    "    \"\"\"\n",
    "    Checks if CUDA is available by attempting to import torch.cuda (if torch is installed)\n",
    "    or by checking for nvidia-smi.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if CUDA is likely available, False otherwise.\n",
    "        str: A string with more details.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return True, f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\"\n",
    "        else:\n",
    "            return False, \"CUDA is not available according to torch.\"\n",
    "    except ImportError:\n",
    "        # torch is not installed, fall back to nvidia-smi\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return True, \"CUDA is likely available (nvidia-smi found), but torch is not installed to confirm.\"\n",
    "        else:\n",
    "            return False, \"CUDA is not available (nvidia-smi not found).\"\n",
    "    except Exception as e:\n",
    "        return False, f\"An unexpected error occurred while checking CUDA availability: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    available, availability_message = check_cuda_availability()\n",
    "    print(f\"CUDA Availability: {available}\")\n",
    "    print(availability_message)\n",
    "\n",
    "    print(\"\\nCUDA Version:\")\n",
    "    print(get_cuda_version())\n",
    "\n",
    "    print(\"\\nNVIDIA-SMI Output:\")\n",
    "    print(get_cuda_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA cache cleaned (Linux/macOS): /home/jags/.nv/ComputeCache\n",
      "CUDA cache directory not found (Linux/macOS): /home/jags/.cache/nvidia/GLCache\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def clean_cuda_cache():\n",
    "    \"\"\"\n",
    "    Attempts to clean the CUDA cache by deleting the shader cache directories.\n",
    "    This is a platform-specific operation and might require administrator privileges.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':  # Windows\n",
    "            cache_dir = os.path.join(os.environ.get('LOCALAPPDATA'), 'NVIDIA', 'DXCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['powershell', '-Command', f'Remove-Item -Recurse -Force \"{cache_dir}\"'], check=True)\n",
    "                print(f\"CUDA cache cleaned (Windows): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Windows): {cache_dir}\")\n",
    "\n",
    "            cache_dir = os.path.join(os.environ.get('LOCALAPPDATA'), 'NVIDIA', 'GLCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['powershell', '-Command', f'Remove-Item -Recurse -Force \"{cache_dir}\"'], check=True)\n",
    "                print(f\"CUDA cache cleaned (Windows): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Windows): {cache_dir}\")\n",
    "\n",
    "        elif os.name == 'posix':  # Linux/macOS\n",
    "            cache_dir = os.path.expanduser('~/.nv/ComputeCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['rm', '-rf', cache_dir], check=True)\n",
    "                print(f\"CUDA cache cleaned (Linux/macOS): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Linux/macOS): {cache_dir}\")\n",
    "\n",
    "            cache_dir = os.path.expanduser('~/.cache/nvidia/GLCache')\n",
    "            if os.path.exists(cache_dir):\n",
    "                subprocess.run(['rm', '-rf', cache_dir], check=True)\n",
    "                print(f\"CUDA cache cleaned (Linux/macOS): {cache_dir}\")\n",
    "            else:\n",
    "                print(f\"CUDA cache directory not found (Linux/macOS): {cache_dir}\")\n",
    "\n",
    "        else:\n",
    "            print(\"Unsupported operating system.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cleaning CUDA cache: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"rm or powershell not found\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied when trying to delete the cache. You may need administrator privileges.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_cuda_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prerequisite. Run on Terminal. Install miniconda. Navigate to the folder cd /home/asus/\n",
    "\n",
    "# mkdir -p ~/miniconda3\n",
    "# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "# bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "# rm -rf ~/miniconda3/miniconda.sh\n",
    "# ~/miniconda3/bin/conda init bash\n",
    "# ~/miniconda3/bin/conda init zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUn on terminal: Create a virtual environment, Install all the requirements like pytorch, xformers, cuda etc.\n",
    "#### install the unsloth library and other libraries viz. peft\n",
    "\n",
    "# conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "# conda activate unsloth_env\n",
    "# pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Mistral patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that\n",
    "includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    " Highlight the missing information to the reporter: Explicitly notify the user which information is missing.\n",
    "\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for summary, report in zip(examples[\"LLAMA Output\"], examples[\"Input\"]):\n",
    "        formatted_text = alpaca_prompt.format(Summary=summary,Response=report) + EOS_TOKEN\n",
    "        texts.append(formatted_text)\n",
    "    return {\"text\": texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    \n",
    "    for summary, report in zip(examples[\"NEW_llama_output\"], examples[\"text\"]):\n",
    "        # Create the conversation in the required JSON format\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in generating detailed bug reports.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"### Instruction:\n",
    "Please create a bug report that\n",
    "includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter: Explicitly notify the user which information is missing.\n",
    "\n",
    "### Context:\n",
    "{summary}\"\"\"},\n",
    "            {\"role\": \"assistant\", \"content\": report}\n",
    "        ]\n",
    "        \n",
    "        # Convert each message to a JSON string and join with newlines\n",
    "        formatted_text = \"\\n\".join(json.dumps(message) for message in conversation)\n",
    "        texts.append(formatted_text)\n",
    "    \n",
    "    return {\"train_text\": texts}\n",
    "\n",
    "# Define the EOS token to be used with the tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt= \"\"\"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\n",
    "\n",
    "# ### sql_prompt:\n",
    "# {}\n",
    "\n",
    "# ### sql:\n",
    "# {}\n",
    "\n",
    "# ### Explanation:\n",
    "# {}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an assistant specialized in generating detailed bug reports.\\n\\n### Instruction:\\nPlease create a bug report that\\nincludes the following sections:\\n1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\\n2. Expected Result (ER): What you expected to happen.\\n3. Actual Result (AR): What actually happened.\\n4. Additional Information: Include relevant details such as software version, build number, environment, etc.\\n Highlight the missing information to the reporter: Explicitly notify the user which information is missing.\\n\\n### Context:\\n{Summary}\\n\\n### Response:\\n{Response}\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_prompts_func(examples):\n",
    "#     sql_prompt = examples[\"sql_prompt\"]\n",
    "#     sql = examples[\"sql\"]\n",
    "#     sql_explanation = examples['sql_explanation']\n",
    "\n",
    "#     texts = []\n",
    "    \n",
    "#     EOS_TOKEN = tokenizer.eos_token\n",
    "#     for sql_prompt, sql, sql_explanation in zip(sql_prompt, sql, sql_explanation):\n",
    "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#         text = alpaca_prompt.format(sql_prompt, sql, sql_explanation) + EOS_TOKEN\n",
    "#         texts.append(text)\n",
    "#     return { \"text\" : texts, }\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'user_behviou', 'total_score', 'max_possible', 'score_percentage', 'RM1_size_passed', 'RM1_size_score', 'RM2_readability_passed', 'RM2_readability_score', 'RM3_punctuation_passed', 'RM3_punctuation_score', 'RM4_sentence_length_passed', 'RM4_sentence_length_score', 'RR1_itemization_passed', 'RR1_itemization_score', 'RR2_itemization_symbol_passed', 'RR2_itemization_symbol_score', 'RR3_environment_passed', 'RR3_environment_score', 'RR4_screenshot_passed', 'RR4_screenshot_score', 'RR5_screenshot_guideline_passed', 'RR5_screenshot_guideline_score', 'RA1_interface_element_passed', 'RA1_interface_element_score', 'RA2_user_behavior_passed', 'RA2_user_behavior_score', 'RA3_system_defect_passed', 'RA3_system_defect_score', 'RA4_defect_description_passed', 'RA4_defect_description_score', 'NEW_llama_output'],\n",
       "    num_rows: 3172\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'user_behviou', 'total_score', 'max_possible', 'score_percentage', 'RM1_size_passed', 'RM1_size_score', 'RM2_readability_passed', 'RM2_readability_score', 'RM3_punctuation_passed', 'RM3_punctuation_score', 'RM4_sentence_length_passed', 'RM4_sentence_length_score', 'RR1_itemization_passed', 'RR1_itemization_score', 'RR2_itemization_symbol_passed', 'RR2_itemization_symbol_score', 'RR3_environment_passed', 'RR3_environment_score', 'RR4_screenshot_passed', 'RR4_screenshot_score', 'RR5_screenshot_guideline_passed', 'RR5_screenshot_guideline_score', 'RA1_interface_element_passed', 'RA1_interface_element_score', 'RA2_user_behavior_passed', 'RA2_user_behavior_score', 'RA3_system_defect_passed', 'RA3_system_defect_score', 'RA4_defect_description_passed', 'RA4_defect_description_score', 'NEW_llama_output', 'train_text'],\n",
       "    num_rows: 3172\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"role\": \"system\", \"content\": \"You are an assistant specialized in generating detailed bug reports.\"}\\n{\"role\": \"user\", \"content\": \"### Instruction:\\\\nPlease create a bug report that\\\\nincludes the following sections:\\\\n1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\\\\n2. Expected Result (ER): What you expected to happen.\\\\n3. Actual Result (AR): What actually happened.\\\\n4. Additional Information: Include relevant details such as software version, build number, environment, etc.\\\\nHighlight the missing information to the reporter: Explicitly notify the user which information is missing.\\\\n\\\\n### Context:\\\\nHere\\'s the rewritten paragraph:\\\\n\\\\n\\\\\"I ran into an issue with Firefox on my Windows 10 machine (x64) and wanted to report it. When I opened the browser, typed in \\'about:crashparent\\' and navigated to the relevant page, I expected the \\\\\"Quit Firefox\\\\\" and \\\\\"Restart Firefox\\\\\" buttons to be in the same position as usual. But instead, they\\'re reversed - the \\\\\"Quit Firefox\\\\\" button is now on the right, and the \\\\\"Restart Firefox\\\\\" one is on the left! This happened in version 126.0a1 (dated April 11th). I tested it on my Mac (OS 14) and Ubuntu machine (23.10), but only saw this issue on Windows. You can check out the pushlog for more context, and I think Bug #1887503 might be related to this problem. I\\'ve attached a screenshot of what I\\'m seeing - let me know if you need any more info!\\\\\"\"}\\n{\"role\": \"assistant\", \"content\": \"created attachment 9396376\\\\nimage.png\\\\n\\\\n**found in**\\\\n* 126.0a1 (2024-04-11)\\\\n\\\\n**affected versions**\\\\n* 126.0a1 (2024-04-11)\\\\n\\\\n**tested platforms**\\\\n* affected platforms: windows 10x64\\\\n* unaffected platforms: macos 14, ubuntu 23.10\\\\n\\\\n**steps to reproduce**\\\\n1. open firefox and type about:crashparent.\\\\n\\\\n**expected result** \\\\n* the quit firefox and restart firefox buttons have the same position as before (restart firefox, quit firefox)\\\\n\\\\n**actual result**\\\\n* the quit firefox and restart firefox buttons are reversed (quit firefox, restart firefox).\\\\n\\\\n\\\\n**regression range**\\\\n*pushlog: https://hg.mozilla.org/mozilla-central/pushloghtml?fromchange=f8ac03b3c764017d5704deed413c8d35c99978fd&tochange=6109c9e63027ad132c8448ca4457d263284fc283 \\\\npossible regressor: bug 1887503\\\\n\\\\n**additional notes**\\\\n* attached a screenshot.\"}']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train_text\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change as per paper\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"train_text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 3,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps =  120,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.988 GB.\n",
      "8.156 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457.1889 seconds used for training.\n",
      "24.29 minutes used for training.\n",
      "Peak reserved memory = 8.779 GB.\n",
      "Peak reserved memory for training = 0.623 GB.\n",
      "Peak reserved memory % of max memory = 36.597 %.\n",
      "Peak reserved memory for training % of max memory = 2.597 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id_x', 'count', 'id', 'tags', 'reactions', 'is_private', 'raw_text', 'creation_time_x', 'text', 'bug_id', 'attachment_id', 'creator_x', 'time', 'author', 'Bug report', 'author_id', '_id_y', 'severity', 'summary', 'votes', 'cf_tracking_firefox_esr128', 'qa_contact', 'cf_fx_points', 'cf_last_resolved', 'duplicates', 'see_also', 'cf_tracking_thunderbird_esr128', 'cf_tracking_thunderbird_esr115', 'cc_detail', 'cf_qa_whiteboard', 'blocks', 'cf_webcompat_score', 'product', 'flags', 'alias', 'creation_time_y', 'cf_has_str', 'mentors', 'creator_detail', 'cf_status_thunderbird_esr128', 'cf_crash_signature', 'op_sys', 'cf_accessibility_severity', 'target_milestone', 'url', 'cf_user_story', 'cf_tracking_firefox_relnote', 'cf_webcompat_priority', 'priority', 'cf_fx_iteration', 'cf_a11y_review_project_flag', 'is_open', 'cf_tracking_firefox132', 'status', 'cf_cab_review', 'cf_status_firefox134', 'assigned_to_detail', 'cf_tracking_firefox134', 'version', 'keywords', 'cf_status_firefox_esr115', 'is_confirmed', 'mentors_detail', 'component', 'cc', 'comment_count', 'cf_performance_impact', 'cf_status_firefox132', 'type', 'is_cc_accessible', 'dupe_of', 'regressed_by', 'whiteboard', 'platform', 'cf_status_firefox133', 'cf_tracking_firefox133', 'groups', 'cf_tracking_firefox_esr115', 'creator_y', 'cf_status_firefox_esr128', 'resolution', 'last_change_time', 'assigned_to', 'cf_status_thunderbird_esr115', 'classification', 'cf_rank', 'depends_on', 'regressions', 'is_creator_accessible', 'Contributor_email', 'Contributor_Id', 'cf_status_firefox_esr102', 'cf_status_firefox109', 'cf_status_firefox108', 'cf_status_firefox107', 'cf_status_firefox118', 'cf_status_firefox119', 'cf_tracking_firefox119', 'cf_tracking_firefox120', 'cf_status_firefox120', 'cf_blocking_webextensions', 'cf_tracking_thunderbird_134', 'cf_tracking_thunderbird_relnote', 'cf_tracking_thunderbird_132', 'cf_status_thunderbird_132', 'cf_status_thunderbird_133', 'cf_tracking_thunderbird_133', 'cf_status_thunderbird_134', 'cf_tracking_firefox109', 'cf_status_firefox_esr78', 'cf_status_firefox88', 'cf_status_thunderbird_esr78', 'cf_status_firefox89', 'cf_status_firefox87', 'cf_install_update_workflow', 'cf_tracking_firefox89', 'cf_status_firefox86', 'cf_due_date', 'cf_tracking_bmo_push', 'cf_status_firefox110', 'cf_fission_milestone', 'cf_locale', 'cf_status_firefox111', 'cf_status_firefox90', 'cf_status_firefox91', 'cf_status_firefox112', 'cf_status_seamonkey257esr', 'cf_tracking_seamonkey257esr', 'cf_status_seamonkey253', 'cf_status_thunderbird_88', 'cf_tracking_seamonkey253', 'cf_status_firefox114', 'cf_status_firefox115', 'cf_status_firefox113', 'cf_status_firefox117', 'cf_status_firefox97', 'qa_contact_detail', 'cf_status_thunderbird_89', 'cf_tracking_thunderbird_esr78', 'cf_tracking_firefox107', 'cf_tracking_conduit_push', 'cf_status_conduit_push', 'cf_status_firefox129', 'cf_status_firefox131', 'cf_status_firefox130', 'cf_status_firefox94', 'cf_status_firefox96', 'cf_status_firefox_esr91', 'cf_status_firefox95', 'cf_status_thunderbird_esr102', 'cf_status_thunderbird_109', 'cf_status_firefox116', 'cf_tracking_firefox88', 'cf_tracking_firefox108', 'cf_tracking_firefox_esr102', 'cf_status_thunderbird_108', 'cf_status_bmo_push', 'cf_tracking_thunderbird_108', 'cf_tracking_thunderbird_esr102', 'cf_status_firefox123', 'cf_status_thunderbird_115', 'cf_tracking_firefox110', 'cf_status_thunderbird_esr91', 'cf_status_thunderbird_99', 'cf_status_thunderbird_111', 'cf_status_firefox93', 'cf_status_firefox92', 'cf_status_thunderbird_87', 'cf_status_firefox100', 'cf_tracking_firefox87', 'cf_tracking_firefox_esr78', 'cf_status_firefox121', 'cf_status_firefox122', 'cf_status_firefox103', 'cf_status_firefox104', 'cf_status_firefox126', 'cf_tracking_firefox90', 'cf_tracking_firefox91', 'cf_status_thunderbird_123', 'cf_status_firefox128', 'cf_tracking_fxios', 'cf_tracking_firefox121', 'cf_status_firefox125', 'cf_status_firefox106', 'cf_status_firefox105', 'cf_status_firefox124', 'cf_status_thunderbird_90', 'cf_status_thunderbird_91', 'cf_status_thunderbird_121', 'cf_status_thunderbird_122', 'cf_tracking_firefox111', 'cf_status_firefox99', 'cf_status_firefox101', 'cf_status_firefox102', 'cf_status_firefox98', 'cf_tracking_thunderbird_88', 'cf_status_thunderbird_107', 'cf_tracking_firefox86', 'cf_status_thunderbird_93', 'cf_status_thunderbird_94', 'cf_tracking_thunderbird_esr91', 'cf_tracking_firefox_esr91', 'cf_status_thunderbird_96', 'cf_status_thunderbird_97', 'cf_tracking_firefox112', 'cf_status_thunderbird_110', 'cf_status_thunderbird_86', 'cf_tracking_firefox_sumo', 'cf_status_thunderbird_105', 'cf_status_firefox127', 'cf_status_thunderbird_101', 'cf_tracking_firefox106', 'cf_tracking_thunderbird_115', 'cf_tracking_firefox100', 'cf_tracking_firefox101', 'cf_tracking_firefox95', 'cf_tracking_thunderbird_87', 'cf_status_thunderbird_100', 'cf_tracking_thunderbird_102', 'cf_status_thunderbird_102', 'cf_tracking_firefox97', 'cf_tracking_firefox98', 'cf_tracking_firefox96', 'cf_status_thunderbird_103', 'cf_status_thunderbird_104', 'cf_tracking_thunderbird_91', 'cf_status_thunderbird_98', 'cf_tracking_firefox99', 'cf_tracking_thunderbird_97', 'cf_status_firefox85', 'cf_colo_site', 'cf_tracking_thunderbird_90', 'cf_tracking_firefox102', 'cf_tracking_thunderbird_93', 'cf_tracking_thunderbird_96', 'cf_status_thunderbird_113', 'cf_tracking_thunderbird_86', 'cf_status_thunderbird_85', 'cf_tracking_thunderbird_101', 'cf_status_thunderbird_120', 'cf_status_thunderbird_119', 'cf_status_thunderbird_106', 'cf_tracking_firefox85', 'cf_tracking_firefox113', 'cf_mozilla_project', 'cf_status_thunderbird_92', 'cf_tracking_firefox105', 'cf_tracking_thunderbird_131', 'cf_status_thunderbird_114', 'cf_tracking_thunderbird_130', 'cf_status_firefox84', 'cf_tracking_firefox103', 'cf_status_thunderbird_117', 'cf_status_thunderbird_116', 'cf_status_thunderbird_112', 'cf_tracking_thunderbird_100', 'cf_tracking_thunderbird_99', 'cf_tracking_firefox94', 'cf_tracking_firefox93', 'cf_status_thunderbird_95', 'cf_tracking_thunderbird_94', 'cf_status_thunderbird_118', 'cf_tracking_firefox92', 'cf_tracking_firefox127', 'cf_tracking_thunderbird_106', 'cf_tracking_thunderbird_95', 'cf_status_thunderbird_124', 'cf_tracking_thunderbird_105', 'cf_tracking_firefox104', 'cf_tracking_thunderbird_116', 'cf_tracking_thunderbird_92', 'cf_tracking_thunderbird_103', 'cf_tracking_thunderbird_128', 'cf_status_thunderbird_128', 'cf_tracking_thunderbird_122', 'cf_tracking_thunderbird_109', 'cf_tracking_firefox128', 'cf_tracking_firefox126', 'cf_tracking_firefox122', 'cf_tracking_thunderbird_104', 'cf_tracking_firefox115', 'cf_tracking_firefox116', 'cf_tracking_firefox117', 'cf_status_thunderbird_129', 'cf_tracking_firefox124', 'cf_tracking_thunderbird_111', 'cf_tracking_thunderbird_89', 'cf_root_cause', 'cf_tracking_firefox114', 'cf_tracking_firefox118', 'cf_tracking_thunderbird_119', 'cf_status_thunderbird_131', 'cf_tracking_firefox129', 'cf_tracking_thunderbird_127', 'cf_status_thunderbird_127', 'cf_status_thunderbird_126', 'cf_status_thunderbird_130', 'cf_status_thunderbird_125', 'cf_tracking_thunderbird_126', 'cf_tracking_firefox125', 'cf_tracking_thunderbird_125', 'cf_tracking_thunderbird_124', 'cf_tracking_firefox123', 'cf_tracking_thunderbird_129', 'cf_tracking_thunderbird_123', 'cf_tracking_thunderbird_121', 'cf_tracking_thunderbird_120', 'cf_tracking_thunderbird_118', 'cf_tracking_thunderbird_117', 'cf_tracking_thunderbird_114', 'cf_tracking_thunderbird_113', 'cf_tracking_thunderbird_112', 'cf_tracking_thunderbird_110', 'cf_tracking_firefox130', 'contains_steps_to_reproduce', 'total_score', 'max_possible', 'score_percentage', 'RM1_size_passed', 'RM1_size_score', 'RM2_readability_passed', 'RM2_readability_score', 'RM3_punctuation_passed', 'RM3_punctuation_score', 'RM4_sentence_length_passed', 'RM4_sentence_length_score', 'RR1_itemization_passed', 'RR1_itemization_score', 'RR2_itemization_symbol_passed', 'RR2_itemization_symbol_score', 'RR3_environment_passed', 'RR3_environment_score', 'RR4_screenshot_passed', 'RR4_screenshot_score', 'RR5_screenshot_guideline_passed', 'RR5_screenshot_guideline_score', 'RA1_interface_element_passed', 'RA1_interface_element_score', 'RA2_user_behavior_passed', 'RA2_user_behavior_score', 'RA3_system_defect_passed', 'RA3_system_defect_score', 'RA4_defect_description_passed', 'RA4_defect_description_score', 'Bug ID', 'Comment ID', 'Author', 'Comment Text', 'Contains Steps to Reproduce', 'Contains Actual Result', 'Contains Expected Result or Expect', 'Contains Expected', 'Contains Actual', 'Steps to Reproduce', 'Expected_Res', 'Actual_Res', 'Summary', 'Product', '__index_level_0__', 'NEW_llama_output'],\n",
       "    num_rows: 1027\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"jag2023/Ultimate_CTQRS_good_BG\", split = \"train\")\n",
    "test_dataset = load_dataset(\"jag2023/Ultimate_CTQRS_good_BG\", split = \"test\")\n",
    "validation = load_dataset(\"jag2023/Ultimate_CTQRS_good_BG\", split = \"validation\")\n",
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer([prompt], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Generate the output\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Decode the output\u001b[39;00m\n\u001b[1;32m     50\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1508\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1508\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/peft/peft_model.py:1838\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1837\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1840\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/mistral.py:213\u001b[0m, in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    222\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    223\u001b[0m         causal_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    232\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    923\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    924\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 925\u001b[0m hidden_states, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    935\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:262\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    260\u001b[0m     A \u001b[38;5;241m=\u001b[39m torch_matmul(Qn, Knn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m), out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention[:,:,:,:cached_len])\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# if attention_mask is not None: A += attention_mask # Must add attention_mask for batched\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     A[:] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_nn_functional_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.to(A.dtype)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     A \u001b[38;5;241m=\u001b[39m torch_matmul(A, Vnn, out \u001b[38;5;241m=\u001b[39m Qn)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/functional.py:2142\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an senior engineer specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report with proper itemization and it should includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = test_dataset.to_pandas()\n",
    "dataset= dataset[:200]\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['NEW_llama_output']\n",
    "    actual_report = row['text']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune mistral Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"RQ1CTQRS_200_Score_test_mistral.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "index 7\n",
      "index 8\n",
      "index 9\n",
      "index 10\n",
      "index 11\n",
      "index 12\n",
      "index 13\n",
      "index 14\n",
      "index 15\n",
      "index 16\n",
      "index 17\n",
      "index 18\n",
      "index 19\n",
      "index 20\n",
      "index 21\n",
      "index 22\n",
      "index 23\n",
      "index 24\n",
      "index 25\n",
      "index 26\n",
      "index 27\n",
      "index 28\n",
      "index 29\n",
      "index 30\n",
      "index 31\n",
      "index 32\n",
      "index 33\n",
      "index 34\n",
      "index 35\n",
      "index 36\n",
      "index 37\n",
      "index 38\n",
      "index 39\n",
      "index 40\n",
      "index 41\n",
      "index 42\n",
      "index 43\n",
      "index 44\n",
      "index 45\n",
      "index 46\n",
      "index 47\n",
      "index 48\n",
      "index 49\n",
      "index 50\n",
      "index 51\n",
      "index 52\n",
      "index 53\n",
      "index 54\n",
      "index 55\n",
      "index 56\n",
      "index 57\n",
      "index 58\n",
      "index 59\n",
      "index 60\n",
      "index 61\n",
      "index 62\n",
      "index 63\n",
      "index 64\n",
      "index 65\n",
      "index 66\n",
      "index 67\n",
      "index 68\n",
      "index 69\n",
      "index 70\n",
      "index 71\n",
      "index 72\n",
      "index 73\n",
      "index 74\n",
      "index 75\n",
      "index 76\n",
      "index 77\n",
      "index 78\n",
      "index 79\n",
      "index 80\n",
      "index 81\n",
      "index 82\n",
      "index 83\n",
      "index 84\n",
      "index 85\n",
      "index 86\n",
      "index 87\n",
      "index 88\n",
      "index 89\n",
      "index 90\n",
      "index 91\n",
      "index 92\n",
      "index 93\n",
      "index 94\n",
      "index 95\n",
      "index 96\n",
      "index 97\n",
      "index 98\n",
      "index 99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df\n",
    "# dataset = dataset[:10]\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary_S2r = row['Summary_S2R']\n",
    "    summary_expected = row['Summary_Expected']\n",
    "    summary_actual = row['Summary_Actual']\n",
    "    actual_report = row['raw_text']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary_S2r, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output_S2R = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary_expected, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output_expected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary_actual, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output_actual = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output_S2R.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response_s2r = decoded_output_S2R[response_start:].strip()\n",
    "    response_start = decoded_output_expected.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response_expected = decoded_output_expected[response_start:].strip()\n",
    "    response_start = decoded_output_actual.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response_actual = decoded_output_actual[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        'S2R_masked_output_mistral': generated_response_s2r,\n",
    "        'expected_masked_output_mistral': generated_response_expected,\n",
    "        'actual_masked_output_mistral': generated_response_actual,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"RQ2_all_3_masked_Reports.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "index 7\n",
      "index 8\n",
      "index 9\n",
      "index 10\n",
      "index 11\n",
      "index 12\n",
      "index 13\n",
      "index 14\n",
      "index 15\n",
      "index 16\n",
      "index 17\n",
      "index 18\n",
      "index 19\n",
      "index 20\n",
      "index 21\n",
      "index 22\n",
      "index 23\n",
      "index 24\n",
      "index 25\n",
      "index 26\n",
      "index 27\n",
      "index 28\n",
      "index 29\n",
      "index 30\n",
      "index 31\n",
      "index 32\n",
      "index 33\n",
      "index 34\n",
      "index 35\n",
      "index 36\n",
      "index 37\n",
      "index 38\n",
      "index 39\n",
      "index 40\n",
      "index 41\n",
      "index 42\n",
      "index 43\n",
      "index 44\n",
      "index 45\n",
      "index 46\n",
      "index 47\n",
      "index 48\n",
      "index 49\n",
      "index 50\n",
      "index 51\n",
      "index 52\n",
      "index 53\n",
      "index 54\n",
      "index 55\n",
      "index 56\n",
      "index 57\n",
      "index 58\n",
      "index 59\n",
      "index 60\n",
      "index 61\n",
      "index 62\n",
      "index 63\n",
      "index 64\n",
      "index 65\n",
      "index 66\n",
      "index 67\n",
      "index 68\n",
      "index 69\n",
      "index 70\n",
      "index 71\n",
      "index 72\n",
      "index 73\n",
      "index 74\n",
      "index 75\n",
      "index 76\n",
      "index 77\n",
      "index 78\n",
      "index 79\n",
      "index 80\n",
      "index 81\n",
      "index 82\n",
      "index 83\n",
      "index 84\n",
      "index 85\n",
      "index 86\n",
      "index 87\n",
      "index 88\n",
      "index 89\n",
      "index 90\n",
      "index 91\n",
      "index 92\n",
      "index 93\n",
      "index 94\n",
      "index 95\n",
      "index 96\n",
      "index 97\n",
      "index 98\n",
      "index 99\n",
      "index 100\n",
      "index 101\n",
      "index 102\n",
      "index 103\n",
      "index 104\n",
      "index 105\n",
      "index 106\n",
      "index 107\n",
      "index 108\n",
      "index 109\n",
      "index 110\n",
      "index 111\n",
      "index 112\n",
      "index 113\n",
      "index 114\n",
      "index 115\n",
      "index 116\n",
      "index 117\n",
      "index 118\n",
      "index 119\n",
      "index 120\n",
      "index 121\n",
      "index 122\n",
      "index 123\n",
      "index 124\n",
      "index 125\n",
      "index 126\n",
      "index 127\n",
      "index 128\n",
      "index 129\n",
      "index 130\n",
      "index 131\n",
      "index 132\n",
      "index 133\n",
      "index 134\n",
      "index 135\n",
      "index 136\n",
      "index 137\n",
      "index 138\n",
      "index 139\n",
      "index 140\n",
      "index 141\n",
      "index 142\n",
      "index 143\n",
      "index 144\n",
      "index 145\n",
      "index 146\n",
      "index 147\n",
      "index 148\n",
      "index 149\n",
      "index 150\n",
      "index 151\n",
      "index 152\n",
      "index 153\n",
      "index 154\n",
      "index 155\n",
      "index 156\n",
      "index 157\n",
      "index 158\n",
      "index 159\n",
      "index 160\n",
      "index 161\n",
      "index 162\n",
      "index 163\n",
      "index 164\n",
      "index 165\n",
      "index 166\n",
      "index 167\n",
      "index 168\n",
      "index 169\n",
      "index 170\n",
      "index 171\n",
      "index 172\n",
      "index 173\n",
      "index 174\n",
      "index 175\n",
      "index 176\n",
      "index 177\n",
      "index 178\n",
      "index 179\n",
      "index 180\n",
      "index 181\n",
      "index 182\n",
      "index 183\n",
      "index 184\n",
      "index 185\n",
      "index 186\n",
      "index 187\n",
      "index 188\n",
      "index 189\n",
      "index 190\n",
      "index 191\n",
      "index 192\n",
      "index 193\n",
      "index 194\n",
      "index 195\n",
      "index 196\n",
      "index 197\n",
      "index 198\n",
      "index 199\n",
      "index 200\n",
      "index 201\n",
      "index 202\n",
      "index 203\n",
      "index 204\n",
      "index 205\n",
      "index 206\n",
      "index 207\n",
      "index 208\n",
      "index 209\n",
      "index 210\n",
      "index 211\n",
      "index 212\n",
      "index 213\n",
      "index 214\n",
      "index 215\n",
      "index 216\n",
      "index 217\n",
      "index 218\n",
      "index 219\n",
      "index 220\n",
      "index 221\n",
      "index 222\n",
      "index 223\n",
      "index 224\n",
      "index 225\n",
      "index 226\n",
      "index 227\n",
      "index 228\n",
      "index 229\n",
      "index 230\n",
      "index 231\n",
      "index 232\n",
      "index 233\n",
      "index 234\n",
      "index 235\n",
      "index 236\n",
      "index 237\n",
      "index 238\n",
      "index 239\n",
      "index 240\n",
      "index 241\n",
      "index 242\n",
      "index 243\n",
      "index 244\n",
      "index 245\n",
      "index 246\n",
      "index 247\n",
      "index 248\n",
      "index 249\n",
      "index 250\n",
      "index 251\n",
      "index 252\n",
      "index 253\n",
      "index 254\n",
      "index 255\n",
      "index 256\n",
      "index 257\n",
      "index 258\n",
      "index 259\n",
      "index 260\n",
      "index 261\n",
      "index 262\n",
      "index 263\n",
      "index 264\n",
      "index 265\n",
      "index 266\n",
      "index 267\n",
      "index 268\n",
      "index 269\n",
      "index 270\n",
      "index 271\n",
      "index 272\n",
      "index 273\n",
      "index 274\n",
      "index 275\n",
      "index 276\n",
      "index 277\n",
      "index 278\n",
      "index 279\n",
      "index 280\n",
      "index 281\n",
      "index 282\n",
      "index 283\n",
      "index 284\n",
      "index 285\n",
      "index 286\n",
      "index 287\n",
      "index 288\n",
      "index 289\n",
      "index 290\n",
      "index 291\n",
      "index 292\n",
      "index 293\n",
      "index 294\n",
      "index 295\n",
      "index 296\n",
      "index 297\n",
      "index 298\n",
      "index 299\n",
      "index 300\n",
      "index 301\n",
      "index 302\n",
      "index 303\n",
      "index 304\n",
      "index 305\n",
      "index 306\n",
      "index 307\n",
      "index 308\n",
      "index 309\n",
      "index 310\n",
      "index 311\n",
      "index 312\n",
      "index 313\n",
      "index 314\n",
      "index 315\n",
      "index 316\n",
      "index 317\n",
      "index 318\n",
      "index 319\n",
      "index 320\n",
      "index 321\n",
      "index 322\n",
      "index 323\n",
      "index 324\n",
      "index 325\n",
      "index 326\n",
      "index 327\n",
      "index 328\n",
      "index 329\n",
      "index 330\n",
      "index 331\n",
      "index 332\n",
      "index 333\n",
      "index 334\n",
      "index 335\n",
      "index 336\n",
      "index 337\n",
      "index 338\n",
      "index 339\n",
      "index 340\n",
      "index 341\n",
      "index 342\n",
      "index 343\n",
      "index 344\n",
      "index 345\n",
      "index 346\n",
      "index 347\n",
      "index 348\n",
      "index 349\n",
      "index 350\n",
      "index 351\n",
      "index 352\n",
      "index 353\n",
      "index 354\n",
      "index 355\n",
      "index 356\n",
      "index 357\n",
      "index 358\n",
      "index 359\n",
      "index 360\n",
      "index 361\n",
      "index 362\n",
      "index 363\n",
      "index 364\n",
      "index 365\n",
      "index 366\n",
      "index 367\n",
      "index 368\n",
      "index 369\n",
      "index 370\n",
      "index 371\n",
      "index 372\n",
      "index 373\n",
      "index 374\n",
      "index 375\n",
      "index 376\n",
      "index 377\n",
      "index 378\n",
      "index 379\n",
      "index 380\n",
      "index 381\n",
      "index 382\n",
      "index 383\n",
      "index 384\n",
      "index 385\n",
      "index 386\n",
      "index 387\n",
      "index 388\n",
      "index 389\n",
      "index 390\n",
      "index 391\n",
      "index 392\n",
      "index 393\n",
      "index 394\n",
      "index 395\n",
      "index 396\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = '''You are an senior engineer specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report with proper itemization and it should includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "respond in JSON format as follows :\n",
    "{{\"id\": \"\", \"title\": \"\", \" description \": \"\", \"AR\": \"\", \"ER\": \"\", \"S2R\": \"\", \"Additional Information\":\"\" }}\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "'''\n",
    "\n",
    "# Load the dataset\n",
    "dataset = test_dataset.to_pandas()\n",
    "# dataset= dataset[:200]\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['NEW_llama_output']\n",
    "    actual_report = row['text']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune mistral Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"RQ2_mapping_Mistral.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "index 7\n",
      "index 8\n",
      "index 9\n",
      "index 10\n",
      "index 11\n",
      "index 12\n",
      "index 13\n",
      "index 14\n",
      "index 15\n",
      "index 16\n",
      "index 17\n",
      "index 18\n",
      "index 19\n",
      "index 20\n",
      "index 21\n",
      "index 22\n",
      "index 23\n",
      "index 24\n",
      "index 25\n",
      "index 26\n",
      "index 27\n",
      "index 28\n",
      "index 29\n",
      "index 30\n",
      "index 31\n",
      "index 32\n",
      "index 33\n",
      "index 34\n",
      "index 35\n",
      "index 36\n",
      "index 37\n",
      "index 38\n",
      "index 39\n",
      "index 40\n",
      "index 41\n",
      "index 42\n",
      "index 43\n",
      "index 44\n",
      "index 45\n",
      "index 46\n",
      "index 47\n",
      "index 48\n",
      "index 49\n",
      "index 50\n",
      "index 51\n",
      "index 52\n",
      "index 53\n",
      "index 54\n",
      "index 55\n",
      "index 56\n",
      "index 57\n",
      "index 58\n",
      "index 59\n",
      "index 60\n",
      "index 61\n",
      "index 62\n",
      "index 63\n",
      "index 64\n",
      "index 65\n",
      "index 66\n",
      "index 67\n",
      "index 68\n",
      "index 69\n",
      "index 70\n",
      "index 71\n",
      "index 72\n",
      "index 73\n",
      "index 74\n",
      "index 75\n",
      "index 76\n",
      "index 77\n",
      "index 78\n",
      "index 79\n",
      "index 80\n",
      "index 81\n",
      "index 82\n",
      "index 83\n",
      "index 84\n",
      "index 85\n",
      "index 86\n",
      "index 87\n",
      "index 88\n",
      "index 89\n",
      "index 90\n",
      "index 91\n",
      "index 92\n",
      "index 93\n",
      "index 94\n",
      "index 95\n",
      "index 96\n",
      "index 97\n",
      "index 98\n",
      "index 99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = '''You are an senior engineer specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report with proper itemization and it should includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "'''\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df_cross\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Bug_report']\n",
    "    actual_report = row['Bug_report']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'mistral_cross_project': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"RQ3_mistral_Cross_Project.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df2\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Summary']\n",
    "    actual_report = row['Actual Report']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune llama Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"top_good50_mapping_study.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "index 1\n",
      "index 2\n",
      "index 3\n",
      "index 4\n",
      "index 5\n",
      "index 6\n",
      "index 7\n",
      "index 8\n",
      "index 9\n",
      "index 10\n",
      "index 11\n",
      "index 12\n",
      "index 13\n",
      "index 14\n",
      "index 15\n",
      "index 16\n",
      "index 17\n",
      "index 18\n",
      "index 19\n",
      "index 20\n",
      "index 21\n",
      "index 22\n",
      "index 23\n",
      "index 24\n",
      "index 25\n",
      "index 26\n",
      "index 27\n",
      "index 28\n",
      "index 29\n",
      "index 30\n",
      "index 31\n",
      "index 32\n",
      "index 33\n",
      "index 34\n",
      "index 35\n",
      "index 36\n",
      "index 37\n",
      "index 38\n",
      "index 39\n",
      "index 40\n",
      "index 41\n",
      "index 42\n",
      "index 43\n",
      "index 44\n",
      "index 45\n",
      "index 46\n",
      "index 47\n",
      "index 48\n",
      "index 49\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df2\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Summary']\n",
    "    actual_report = row['Actual Report']\n",
    "    # mistral_repo = row['Mistral Report']\n",
    "    # pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        # 'Mistral Report': mistral_repo,\n",
    "        'agent_Fine_tune llama Output_': generated_response,\n",
    "        # 'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"top_good50_mapping_study.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd # Make sure you have pandas imported\n",
    "\n",
    "# def structure_llm_output(llm_output_text):\n",
    "#     \"\"\"\n",
    "#     Structures the LLM output text to extract sections for bug report.\n",
    "#     ... (rest of your structure_llm_output function code is the same) ...\n",
    "#     \"\"\"\n",
    "#     sections = {\n",
    "#         \"Steps to Reproduce\": None,\n",
    "#         \"Expected Result\": None,\n",
    "#         \"Actual Result\": None,\n",
    "#         \"Additional Information\": None,\n",
    "#     }\n",
    "#     section_headings = list(sections.keys())\n",
    "#     for i, section_name in enumerate(section_headings):\n",
    "#         next_section_heading = section_headings[i+1] if i+1 < len(section_headings) else None\n",
    "#         # start_pattern = re.compile(rf\"{re.escape(section_name)}:\\s*(.*?)(?=(?:\\n{re.escape(section_headings[i+1])}:)|\\Z)\", re.DOTALL)\n",
    "#         start_pattern = re.compile(rf\"{re.escape(section_name)}:\\s*(.*?)(?=(?:\\n{re.escape(section_headings[i+1])}:)|\\Z)\", re.DOTALL)\n",
    "#         match = start_pattern.search(llm_output_text)\n",
    "#         if match:\n",
    "#             sections[section_name] = match.group(1).strip()\n",
    "#     return sections\n",
    "\n",
    "# # Assuming df2 is your DataFrame and 'Fine_tune llama Output' is the column\n",
    "# # --- Process DataFrame ---\n",
    "\n",
    "# # 1. Create a new column to store the structured output (dictionaries)\n",
    "# df2['Structured_Output'] = None # Initialize a new column\n",
    "\n",
    "# # 2. Iterate through each row of df2 and apply structure_llm_output to the 'Fine_tune llama Output'\n",
    "# for index, row in df2.iterrows():\n",
    "#     llm_output_text = row['Fine_tune llama Output'] # Get the string from the current row\n",
    "#     if isinstance(llm_output_text, str): # Important: Check if it's actually a string\n",
    "#         structured_output_dict = structure_llm_output(llm_output_text)\n",
    "#         df2.at[index, 'Structured_Output'] = structured_output_dict # Store the dictionary back in the DataFrame\n",
    "#     else:\n",
    "#         df2.at[index, 'Structured_Output'] = None # Or handle non-string cases as needed, perhaps log a warning\n",
    "\n",
    "# # --- Example of accessing structured data ---\n",
    "# # After processing, each row in df2 will have a dictionary in the 'Structured_Output' column\n",
    "\n",
    "# # Example: Print structured output for the first few rows\n",
    "# print(\"Structured Output for DataFrame:\")\n",
    "# for index, row in df2.head().iterrows(): # Show for the first few rows\n",
    "#     structured_data = row['Structured_Output']\n",
    "#     if structured_data: # Check if structured data was successfully extracted\n",
    "#         print(f\"\\nRow Index: {index}\")\n",
    "#         print(\"Steps:\", structured_data.get(\"Steps to Reproduce\"))\n",
    "#         print(\"Expected:\", structured_data.get(\"Expected Result\"))\n",
    "#         print(\"Actual:\", structured_data.get(\"Actual Result\"))\n",
    "#         print(\"Additional Info:\", structured_data.get(\"Additional Information\"))\n",
    "#     else:\n",
    "#         print(f\"\\nRow Index: {index} - No structured output (perhaps input was not a string or parsing failed)\")\n",
    "#     print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Summary'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 34\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m     actual_report \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Report\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m     mistral_repo \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMistral Report\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Summary'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming model and tokenizer are already loaded and set up, e.g.:\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"model_name\")\n",
    "# model.to(\"cuda\")\n",
    "# FastLanguageModel.for_inference(model)  # If using a specific library for faster inference\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"You are an assistant specialized in generating detailed bug reports.\n",
    "\n",
    "### Instruction:\n",
    "Please create a bug report that includes the following sections:\n",
    "1. Steps to Reproduce (S2R): Detailed steps to replicate the issue.\n",
    "2. Expected Result (ER): What you expected to happen.\n",
    "3. Actual Result (AR): What actually happened.\n",
    "4. Additional Information: Include relevant details such as software version, build number, environment, etc.\n",
    "Highlight the missing information to the reporter if software version, build number, environment, etc not present.\n",
    "### Context:\n",
    "{Summary}\n",
    "\n",
    "### Response:\n",
    "{Response}\n",
    "\"\"\"\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = test_dataset.to_pandas()\n",
    "dataset= df\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    summary = row['Summary']\n",
    "    actual_report = row['Actual Report']\n",
    "    mistral_repo = row['Mistral Report']\n",
    "    pure_llama = row['Output']\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Format the prompt with the summary\n",
    "    prompt = alpaca_prompt.format(Summary=summary, Response=\"\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    import markdown\n",
    "    # html_response = markdown.markdown(decoded_output)\n",
    "    # print(html_response)\n",
    "    \n",
    "    \n",
    "    # # Extract the generated response (the part after \"### Response:\\n\")\n",
    "    response_start = decoded_output.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_response = decoded_output[response_start:].strip()\n",
    "    \n",
    "    # Collect the results\n",
    "    print(\"index\",index)\n",
    "    # print(\"summary\",summary[:10])\n",
    "    results.append({\n",
    "        'Summary': summary,\n",
    "        'Actual Report': actual_report,\n",
    "        'Mistral Report': mistral_repo,\n",
    "        'Fine_tune Output': generated_response,\n",
    "        'Pura llama Output': pure_llama,\n",
    "    })\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel(\"Full_Final_epoc_600_newrefine_llama_Fine_tine_output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "make: Leaving directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "-- The C compiler identification is GNU 13.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The CXX compiler identification is GNU 13.3.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.43.0\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring incomplete, errors occurred!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mCMake Error at /usr/share/cmake-3.28/Modules/FindPackageHandleStandardArgs.cmake:230 (message):\n",
      "  Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR)\n",
      "Call Stack (most recent call first):\n",
      "  /usr/share/cmake-3.28/Modules/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE)\n",
      "  /usr/share/cmake-3.28/Modules/FindCURL.cmake:194 (find_package_handle_standard_args)\n",
      "  common/CMakeLists.txt:88 (find_package)\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "*** Unsloth: Failed compiling llama.cpp using os.system(...) with error 256. Please report this ASAP!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/save.py:1684\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1682\u001b[0m python_install \u001b[38;5;241m=\u001b[39m install_python_non_blocking([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1683\u001b[0m git_clone\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m-> 1684\u001b[0m makefile \u001b[38;5;241m=\u001b[39m \u001b[43minstall_llama_cpp_make_non_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m new_save_directory, old_username \u001b[38;5;241m=\u001b[39m unsloth_save_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   1686\u001b[0m python_install\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/save.py:779\u001b[0m, in \u001b[0;36minstall_llama_cpp_make_non_blocking\u001b[0;34m()\u001b[0m\n\u001b[1;32m    777\u001b[0m check \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Unsloth: Failed compiling llama.cpp using os.system(...) with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please report this ASAP!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# f\"cmake --build llama.cpp/build --config Release -j{psutil.cpu_count()*2} --clean-first --target {' '.join(LLAMA_CPP_TARGETS)}\",\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: *** Unsloth: Failed compiling llama.cpp using os.system(...) with error 256. Please report this ASAP!"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this Steps for Local Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.06 out of 15.47 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 29/32 [00:01<00:00, 28.82it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:01<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "make: Leaving directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n"
     ]
    }
   ],
   "source": [
    "!make clean -C llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
      "make: Leaving directory '/home/jags/research/Final_Paper_fine_tune/llama.cpp'\n"
     ]
    }
   ],
   "source": [
    "!make all -j -C llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gguf in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: protobuf in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (2.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (6.0.2)\n",
      "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jags/miniconda3/envs/unsloth_env/lib/python3.11/site-packages (from gguf) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gguf protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> Q8_0, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> Q8_0, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128001\n",
      "INFO:gguf.vocab:Setting special token type pad to 128255\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:jagunsloth_model_8.gguf: n_tensors = 291, total_size = 8.5G\n",
      "Writing: 100%|███████████████████████████| 8.53G/8.53G [00:37<00:00, 229Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to jagunsloth_model_8.gguf\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py merged_model --outfile jagunsloth_model_8.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALL OLLAMA using Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      "[sudo] password for asus: \n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:285d51ea4f63810cddbc950e114a3f19c58990862cad7135443f0da6c8b9d17f 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 0% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:285d51ea4f63810cddbc950e114a3f19c58990862cad7135443f0da6c8b9d17f 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 100% \n",
      "copying file sha256:0a518f0766fc3484bbc16b6aa4eeec9fc94dad4eb8d30f4ac7759134bcb91a2c 100% \n",
      "copying file sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b 100% \n",
      "copying file sha256:6e1e79b7c44f96217478536d89869fc23c75f2e1f140e3149a9765c8ed1a6b57 100% \n",
      "copying file sha256:e7e4039387ae974f2130df2909b21335624a1bf421b256191ccd6a280a9db0fa 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:6e1e79b7c44f96217478536d89869fc23c75f2e1f140e3149a9765c8ed1a6b57 100% \n",
      "copying file sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:0a518f0766fc3484bbc16b6aa4eeec9fc94dad4eb8d30f4ac7759134bcb91a2c 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:22b3df42c4e872761824da0313e770fdd3641baf4a7808071145362cf8beeaae 100% \n",
      "copying file sha256:303a35b38c7e6a72988c09cc68f4e739b40ded80b0635191e16588b4c801e15e 100% \n",
      "copying file sha256:1b02cb2b24127ff1e955fe55447dabba3f683488e4edb4c2ff005f25333ac197 100% \n",
      "copying file sha256:b2a2038de0fd7c98be5274be297839968015f8cc0d3f99f55abe3cb050a953dd 100% \n",
      "copying file sha256:0a518f0766fc3484bbc16b6aa4eeec9fc94dad4eb8d30f4ac7759134bcb91a2c 100% \n",
      "copying file sha256:146776fce3f6db1103aa6f249e65ee5544c5923ce6f971b092eee79aa6e5d37b 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:22b3df42c4e872761824da0313e770fdd3641baf4a7808071145362cf8beeaae 100% \n",
      "copying file sha256:0895b17d3e55b100438593bc51a32e8f2af507fe50ecda27e04a980fd5be9d48 100% \n",
      "copying file sha256:3c5cf44023714fb39b05e71e425f8d7b92805ff73f7988b083b8c87f0bf87393 100% \n",
      "copying file sha256:6e1e79b7c44f96217478536d89869fc23c75f2e1f140e3149a9765c8ed1a6b57 100% \n",
      "parsing GGUF \u001b[?25h\n",
      "Error: supplied file was not in GGUF format\n"
     ]
    }
   ],
   "source": [
    "!ollama create jags -f Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt1= \"\"\"Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\n",
    "\n",
    "### sql_prompt:\n",
    "{}\n",
    "\n",
    "### sql:\n",
    "{}\n",
    "\n",
    "### Explanation:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the total volume of timber sold by each salesperson, sorted by salesperson?\\n\\n### sql:\\n\\n\\n### Explanation:\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = alpaca_prompt1.format(\n",
    "    query,\n",
    "    \"\",\n",
    "    \"\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that give an sql prompt. Write a response that appropriately completes the request and gives you an sql and the corresponding explanation.\\n\\n### sql_prompt:\\nWhat is the average property size in inclusive housing areas?\\n\\n### sql:\\n\\n\\n### Explanation:\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
